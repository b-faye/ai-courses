{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>1. Building End-to-End Machine Learning Pipelines</h2>**\n",
        "\n",
        "An end-to-end machine learning pipeline automates the entire workflow of a machine learning project from data ingestion to deployment. It typically includes data preprocessing, feature extraction, model training, and evaluation steps. By encapsulating these steps into a pipeline, you ensure that the workflow is reproducible, efficient, and easy to manage."
      ],
      "metadata": {
        "id": "jf030kTQyUml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({\n",
        "    'age': [25, 30, 35, None],\n",
        "    'salary': [50000, 60000, 70000, 80000],\n",
        "    'gender': ['male', 'female', 'female', 'male'],\n",
        "    'purchased': [1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "# Features and target\n",
        "X = data[['age', 'salary', 'gender']]\n",
        "y = data['purchased']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Define pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), ['age', 'salary']),\n",
        "        ('cat', OneHotEncoder(), ['gender'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "96UUgKJciCog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>2. Data Preprocessing and Feature Extraction</h2>**\n",
        "\n",
        "Data preprocessing involves cleaning and transforming raw data into a format suitable for modeling. Feature extraction is a process of selecting and transforming features to improve model performance. This step is crucial for ensuring that the machine learning model learns relevant patterns from the data."
      ],
      "metadata": {
        "id": "Sx1fJHwhic4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({\n",
        "    'age': [25, 30, 35, None],\n",
        "    'salary': [50000, 60000, 70000, 80000],\n",
        "    'gender': ['male', 'female', 'female', 'male']\n",
        "})\n",
        "\n",
        "# Define preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), ['age', 'salary']),\n",
        "        ('cat', OneHotEncoder(), ['gender'])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_data = preprocessor.fit_transform(data)\n",
        "print(processed_data)"
      ],
      "metadata": {
        "id": "H5x7ctu6ioZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>3. Model Training and Evaluation</h2>**"
      ],
      "metadata": {
        "id": "Ej2yH8zjiuL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training involves using the preprocessed data to train a machine learning algorithm. Evaluation is the process of assessing the performance of the trained model using metrics such as accuracy, precision, recall, and F1 score to ensure it meets the desired performance criteria."
      ],
      "metadata": {
        "id": "URnoWcCxmVjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Initialize and train model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "2N6ZZFlamWHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>4. Deployment Considerations</h2>**\n",
        "\n",
        "Deployment refers to the process of integrating a machine learning model into a production environment where it can make predictions on new data. Key considerations include model performance monitoring, scalability, security, and managing model updates."
      ],
      "metadata": {
        "id": "YdNqjw-imb0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(model, 'random_forest_model.pkl')\n",
        "\n",
        "# Load model\n",
        "loaded_model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "# Predict with loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "C4Sx53kTmiyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>5. Setting Up a Flask API</h2>**"
      ],
      "metadata": {
        "id": "mHmkHilxmrVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Example**\n",
        "\n",
        "First, let’s build and save a machine learning model. For this example, we’ll use a simple RandomForestClassifier trained on the Iris dataset."
      ],
      "metadata": {
        "id": "Sl9raPH9nhEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save_model.py\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Save model to file\n",
        "joblib.dump(model, 'random_forest_model.pkl')\n",
        "\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "C0QIP3-InYPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation**\n",
        "\n",
        "First, ensure Flask is installed. You can install it using pip:"
      ],
      "metadata": {
        "id": "imCeeullm6WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Flask"
      ],
      "metadata": {
        "id": "0UOcZTVFmwHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flask API Code**\n",
        "\n",
        "Create a file named app.py for your Flask application. In this file, we will load the model and create endpoints for making predictions."
      ],
      "metadata": {
        "id": "tx3JqrHgm9V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json()  # Get data posted as JSON\n",
        "    features = np.array(data['features']).reshape(1, -1)  # Convert data to numpy array and reshape\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(features)\n",
        "    prediction_proba = model.predict_proba(features)\n",
        "\n",
        "    # Prepare response\n",
        "    response = {\n",
        "        'prediction': int(prediction[0]),\n",
        "        'prediction_proba': prediction_proba.tolist()\n",
        "    }\n",
        "\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "GBXGkYz7mxNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the Server**\n",
        "\n",
        "Run the Flask app using:"
      ],
      "metadata": {
        "id": "z-d9_0aNnoE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python app.py"
      ],
      "metadata": {
        "id": "SmYBagKhnrMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making Requests**\n",
        "\n",
        "You can test the API using tools like curl, Postman, or even Python’s requests library.\n",
        "Example Request with requests:"
      ],
      "metadata": {
        "id": "-08EEYYqn2Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'http://127.0.0.1:5000/predict'\n",
        "data = {'features': [5.1, 3.5, 1.4, 0.2]}  # Example feature values for Iris dataset\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "OLswi-fdn5RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Request with curl:**"
      ],
      "metadata": {
        "id": "kXkfH6CzoAmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -X POST http://127.0.0.1:5000/predict -H \"Content-Type: application/json\" -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}'"
      ],
      "metadata": {
        "id": "AS3-8kPFoCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment Considerations**\n",
        "\n",
        "  * **Hosting**\n",
        "\n",
        "      For deploying Flask applications in a production environment, consider using platforms like:\n",
        "\n",
        "    - **Heroku**: Provides an easy way to deploy Flask apps.\n",
        "    - **AWS Elastic Beanstalk**: For more control and scalability.\n",
        "    - **Google Cloud Run**: For serverless deployment.\n",
        "\n",
        "* **Security**\n",
        "\n",
        "    - **Input Validation**: Ensure that the input data is validated and sanitized.\n",
        "    - **Environment Variables**: Store sensitive information in environment variables rather than hardcoding them.\n",
        "    - **HTTPS**: Use HTTPS for secure communication.\n",
        "\n",
        "* **Monitoring and Scaling**\n",
        "\n",
        "    - **Monitoring**: Implement logging and monitoring to track API performance and errors.\n",
        "    - **Scaling**: Plan for scaling your application based on traffic and load.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "OTk1H4rVoT8V"
      }
    }
  ]
}