{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IQpgkg0Jv1UX",
        "CCdyGtlqwELF",
        "HpB1qUoawVjY",
        "HePolVHCwdFz",
        "_2fUM9Wlwo9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "When working with large-scale machine learning models, data and model parallelism are crucial for efficient training across multiple GPUs or distributed systems. Here’s a comprehensive overview of how to implement data parallelism and model parallelism using PyTorch.\n",
        "Data Parallelism\n",
        "\n",
        "Data parallelism involves splitting the data across multiple GPUs and running the same model on each GPU. PyTorch’s torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel modules facilitate this.\n",
        "\n",
        "**1. Using torch.nn.DataParallel**\n",
        "\n",
        "DataParallel is simpler to use but may have some performance limitations due to Python GIL (Global Interpreter Lock) and is suitable for single-node multi-GPU setups."
      ],
      "metadata": {
        "id": "QMvPw-wr49OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define a simple dataset and model\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.randn(3, 224, 224), torch.tensor(0)\n",
        "\n",
        "model = models.resnet18(pretrained=False)\n",
        "model = nn.DataParallel(model)  # Wrap the model with DataParallel\n",
        "\n",
        "# Define a simple DataLoader\n",
        "dataset = SimpleDataset(size=100)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    for inputs, targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "IiFERtNe4_-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Using torch.nn.parallel.DistributedDataParallel**\n",
        "\n",
        "DistributedDataParallel (DDP) is more efficient and scalable for multi-node, multi-GPU setups. It is preferred over DataParallel for large-scale training.\n",
        "Setting Up Distributed Training\n",
        "\n",
        "To use DDP, you need to initialize a process group and spawn multiple processes. Here’s an example:"
      ],
      "metadata": {
        "id": "RfpG3Nz05KEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import os\n",
        "\n",
        "# Initialize the distributed process group\n",
        "def init_process_group(backend='nccl'):\n",
        "    dist.init_process_group(backend=backend, init_method='env://')\n",
        "\n",
        "def main():\n",
        "    # Initialize distributed process group\n",
        "    init_process_group()\n",
        "\n",
        "    # Define model, dataset, and dataloader\n",
        "    model = models.resnet18(pretrained=False).cuda()\n",
        "    model = DDP(model, device_ids=[torch.cuda.current_device()])\n",
        "\n",
        "    dataset = SimpleDataset(size=100)\n",
        "    sampler = DistributedSampler(dataset)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(5):\n",
        "        sampler.set_epoch(epoch)  # Ensure data is shuffled differently for each epoch\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if dist.get_rank() == 0:\n",
        "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fcywf6Jr5PRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Launching the Distributed Training**\n",
        "\n",
        "Use torch.distributed.launch or torchrun to start the training script across multiple processes."
      ],
      "metadata": {
        "id": "aytbjwHg5W5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using torch.distributed.launch\n",
        "! python -m torch.distributed.launch --nproc_per_node=4 script.py\n",
        "\n",
        "# Using torchrun (recommended)\n",
        "! torchrun --nproc_per_node=4 script.py\n"
      ],
      "metadata": {
        "id": "JKJCZLS55oAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Parallelism**\n",
        "\n",
        "Model parallelism splits a model across multiple GPUs. This is useful for very large models that cannot fit into the memory of a single GPU.\n",
        "Example: Simple Model Parallelism\n",
        "\n",
        "Here’s a simple example where different layers of a model are placed on different GPUs:"
      ],
      "metadata": {
        "id": "lgTtQIsB5w0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class ModelParallel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelParallel, self).__init__()\n",
        "        self.part1 = nn.Linear(1024, 2048).cuda(0)\n",
        "        self.part2 = nn.Linear(2048, 1024).cuda(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.cuda(0)\n",
        "        x = self.part1(x)\n",
        "        x = x.cuda(1)\n",
        "        x = self.part2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "model = ModelParallel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Dummy data\n",
        "inputs = torch.randn(64, 1024)\n",
        "targets = torch.randn(64, 1024)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "wEjPRbwk5zbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}