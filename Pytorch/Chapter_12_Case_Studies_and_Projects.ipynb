{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IQpgkg0Jv1UX",
        "CCdyGtlqwELF",
        "HpB1qUoawVjY",
        "HePolVHCwdFz",
        "_2fUM9Wlwo9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-World Applications of PyTorch**"
      ],
      "metadata": {
        "id": "JWgGU9evAGqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is a versatile and powerful framework used across various real-world applications due to its dynamic computation graph, flexible API, and strong community support. Below are some real-world applications of PyTorch, along with hands-on project ideas and exercises to get you started.\n",
        "\n",
        "\n",
        "1. **Computer Vision**\n",
        "   - **Object Detection**: Detecting and classifying objects in images or video streams. Examples include using models like YOLO, Faster R-CNN, and RetinaNet.\n",
        "   - **Image Segmentation**: Segmenting objects or regions within an image. Applications include medical imaging (e.g., tumor detection) and autonomous vehicles.\n",
        "   - **Image Classification**: Classifying images into predefined categories. Applications range from facial recognition to scene understanding.\n",
        "\n",
        "2. **Natural Language Processing (NLP)**\n",
        "   - **Text Classification**: Categorizing text into classes, such as spam detection or sentiment analysis.\n",
        "   - **Named Entity Recognition (NER)**: Identifying entities like names, dates, and locations in text.\n",
        "   - **Machine Translation**: Translating text from one language to another using models like Transformers and BERT.\n",
        "   - **Text Generation**: Generating coherent and contextually relevant text, used in chatbots and language models like GPT.\n",
        "\n",
        "3. **Reinforcement Learning**\n",
        "   - **Game Playing**: Training agents to play games (e.g., AlphaGo, Atari games) using algorithms like Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO).\n",
        "   - **Robotics**: Training robots to perform tasks in simulated or real environments.\n",
        "\n",
        "4. **Healthcare**\n",
        "   - **Medical Imaging Analysis**: Detecting diseases from medical scans (e.g., X-rays, MRIs) using deep learning models.\n",
        "   - **Drug Discovery**: Predicting molecular properties and interactions to accelerate drug development.\n",
        "\n",
        "5. **Finance**\n",
        "   - **Algorithmic Trading**: Predicting stock prices and executing trades using machine learning models.\n",
        "   - **Fraud Detection**: Identifying fraudulent transactions and activities using anomaly detection models.\n",
        "\n",
        "6. **Speech Processing**\n",
        "   - **Speech Recognition**: Converting spoken language into text, used in virtual assistants and transcription services.\n",
        "   - **Speech Synthesis**: Generating human-like speech from text.\n"
      ],
      "metadata": {
        "id": "KG8Lix_OAIbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hands-On Projects and Exercises**"
      ],
      "metadata": {
        "id": "b6R6xWP3AaUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Image Classification**\n",
        "\n",
        "- Project: Build an image classifier using the CIFAR-10 dataset.\n",
        "- Objective: Train a convolutional neural network (CNN) to classify images into 10 different categories.\n",
        "- Dataset: CIFAR-10 Dataset\n",
        "- Tools: PyTorch, torchvision\n",
        "\n",
        "Example Code:"
      ],
      "metadata": {
        "id": "lfNuJCdJAkI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32*8*8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 32*8*8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "6iYp5rNQAqQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Text Classification**\n",
        "\n",
        "- Project: Implement a sentiment analysis model using the IMDb dataset.\n",
        "- Objective: Build an LSTM-based model to classify movie reviews as positive or negative.\n",
        "- Dataset: IMDb Dataset\n",
        "- Tools: PyTorch, torchtext\n",
        "\n",
        "Example Code:"
      ],
      "metadata": {
        "id": "qZYuS1V6Ar0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Define the model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        x = self.fc(hn[-1])\n",
        "        return x\n",
        "\n",
        "# Tokenization and vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "def yield_tokens(data_iter):\n",
        "    for label, line in data_iter:\n",
        "        yield tokenizer(line)\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def text_pipeline(x):\n",
        "    return vocab(tokenizer(x))\n",
        "\n",
        "def collate_batch(batch):\n",
        "    labels, texts = zip(*batch)\n",
        "    text_list = [torch.tensor(text_pipeline(text)) for text in texts]\n",
        "    return pad_sequence(text_list, batch_first=True), torch.tensor(labels)\n",
        "\n",
        "# Load data\n",
        "train_iter = IMDB(split='train')\n",
        "train_loader = DataLoader(list(train_iter), batch_size=64, collate_fn=collate_batch)\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "output_size = 2  # Positive or Negative\n",
        "model = LSTMClassifier(vocab_size, embed_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "7uvZeIG3A2Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Reinforcement Learning**\n",
        "\n",
        "- Project: Implement a simple reinforcement learning agent to solve the CartPole environment using Q-learning.\n",
        "- Objective: Train an agent to balance a pole on a cart.\n",
        "- Environment: CartPole-v1\n",
        "- Tools: PyTorch, OpenAI Gym\n",
        "\n",
        "Example Code:"
      ],
      "metadata": {
        "id": "_zSCq-uuA6hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training parameters\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "lr = 0.001\n",
        "batch_size = 64\n",
        "memory = deque(maxlen=10000)\n",
        "target_update = 10\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "policy_net = QNetwork(state_size, action_size)\n",
        "target_net = QNetwork(state_size, action_size)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Collect experience\n",
        "def select_action(state):\n",
        "    if random.random() > epsilon:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(torch.tensor(state, dtype=torch.float32)).argmax().item()\n",
        "    else:\n",
        "        return random.randrange(action_size)\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "    transitions = random.sample(memory, batch_size)\n",
        "    batch = namedtuple('Batch', ['state', 'action', 'next_state', 'reward'])\n",
        "    batch = batch(*zip(*transitions))\n",
        "\n",
        "    states = torch.tensor(batch.state, dtype=torch.float32)\n",
        "    actions = torch.tensor(batch.action)\n",
        "    next_states = torch.tensor(batch.next_state, dtype=torch.float32)\n",
        "    rewards = torch.tensor(batch.reward, dtype=torch.float32)\n",
        "\n",
        "    state_action_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "    next_state_values = target_net(next_states).max(1)[0]\n",
        "    expected_state_action_values = rewards + (gamma * next_state_values)\n",
        "\n",
        "    loss = criterion(state_action_values, expected_state_action_values.detach())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    for t in range(1000):\n",
        "        action = select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        memory.append((state, action, next_state, reward))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode % target_update == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "etcxe8GhA_qP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}