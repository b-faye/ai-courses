{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IQpgkg0Jv1UX",
        "CCdyGtlqwELF",
        "HpB1qUoawVjY",
        "HePolVHCwdFz",
        "_2fUM9Wlwo9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. AutoML with PyTorch**\n",
        "\n",
        "**2. PyTorch Lightning**\n",
        "\n",
        "**3. Hyperparameter tuning with PyTorch**"
      ],
      "metadata": {
        "id": "wR_fhhqt6YNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AutoML with PyTorch**"
      ],
      "metadata": {
        "id": "v6SFsoiN6lP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoML (Automated Machine Learning) aims to automate the process of building and tuning machine learning models. While traditional AutoML frameworks offer end-to-end solutions, there are several ways to integrate AutoML principles with PyTorch for tasks like hyperparameter optimization, architecture search, and automated model selection. Here's an overview and examples of using AutoML concepts with PyTorch.\n",
        "\n",
        "**1. Hyperparameter Optimization**\n",
        "\n",
        "Hyperparameter optimization is a crucial part of the AutoML process, as it involves finding the best set of hyperparameters for a model. Several libraries can assist with hyperparameter tuning in PyTorch.\n",
        "\n",
        "**Example with Optuna**\n",
        "\n",
        "**Optuna** is a popular hyperparameter optimization framework that works well with PyTorch."
      ],
      "metadata": {
        "id": "QDJgeLZa7R-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to optimize\n",
        "    hidden_units = trial.suggest_int('hidden_units', 64, 128)\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "\n",
        "    # Load dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    model = SimpleNN(hidden_units).to('cuda')\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(1):\n",
        "        for batch in train_loader:\n",
        "            data, target = batch\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation (using the same dataset here for simplicity)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to('cuda'), target.to('cuda')\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Optimize hyperparameters using Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "print(\"Best accuracy: \", study.best_value)\n"
      ],
      "metadata": {
        "id": "XvSDE4DT7YXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Neural Architecture Search (NAS)**\n",
        "\n",
        "Neural Architecture Search involves finding the best neural network architecture for a given task. While building a NAS system from scratch is complex, there are libraries and frameworks that simplify the process.\n",
        "Example with NAT (Neural Architecture Transformer)\n",
        "\n",
        "NAT is a PyTorch-based library for architecture search."
      ],
      "metadata": {
        "id": "-AncG1UA7aXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code snippet for using a NAS library like NAT\n",
        "from nat import NAT\n",
        "\n",
        "# Define a search space and a model\n",
        "search_space = {\n",
        "    'layers': [2, 3, 4],\n",
        "    'units': [64, 128, 256],\n",
        "}\n",
        "\n",
        "# Initialize NAT\n",
        "nas = NAT(search_space=search_space, model_class=SimpleNN)\n",
        "\n",
        "# Search for the best architecture\n",
        "best_architecture = nas.search()\n",
        "print(\"Best architecture: \", best_architecture)"
      ],
      "metadata": {
        "id": "-l5Fi01e7hux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: This is a conceptual example. For actual implementation, refer to specific NAS libraries' documentation."
      ],
      "metadata": {
        "id": "gUXM8ScV706S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. AutoML Frameworks Integrating with PyTorch**\n",
        "\n",
        "Several AutoML frameworks integrate with PyTorch to automate model building and hyperparameter tuning.\n",
        "Example with AutoKeras\n",
        "\n",
        "AutoKeras is an AutoML library that works well with PyTorch and TensorFlow. Here’s how you can use it with PyTorch:"
      ],
      "metadata": {
        "id": "-BNs-W2F71Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import autokeras as ak\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Initialize AutoKeras ImageClassifier\n",
        "clf = ak.ImageClassifier(max_trials=3, overwrite=True)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = clf.evaluate(x_test, y_test)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "f6NHTgoY77xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Automated Model Selection**\n",
        "\n",
        "Automated model selection involves choosing the best model architecture and configuration from a set of candidates. This can be done using libraries like Optuna in conjunction with model evaluation functions.\n",
        "Example: Automated Model Selection with Optuna"
      ],
      "metadata": {
        "id": "kqb_GXXl8BIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define various models\n",
        "class ModelA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelA, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ModelB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelB, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.fc1 = nn.Linear(32 * 26 * 26, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = x.view(-1, 32 * 26 * 26)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    model_type = trial.suggest_categorical('model_type', [ModelA, ModelB])\n",
        "    model = model_type().to('cuda')\n",
        "\n",
        "    # Hyperparameters\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
        "\n",
        "    # Load dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Training setup\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(1):\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Perform optimization\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "print(\"Best accuracy: \", study.best_value)"
      ],
      "metadata": {
        "id": "Pc5ra-kk8DCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PyTorch Lightning**"
      ],
      "metadata": {
        "id": "Q9sm14D78JDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Lightning is a high-level library built on top of PyTorch that aims to simplify the training process by abstracting boilerplate code, making your PyTorch code more readable and maintainable. It provides a standardized way to organize PyTorch code and helps with tasks like training loops, validation, and checkpointing, allowing you to focus more on the research and model development.\n",
        "Key Features of PyTorch Lightning\n",
        "\n",
        "- Simplified Code Structure: Encapsulates common training practices into reusable components.\n",
        "- Automatic Handling of Training Loops: Manages training, validation, and testing loops.\n",
        "- Seamless Multi-GPU and Distributed Training: Easy integration with multi-GPU and distributed setups.\n",
        "- Integrated Logging and Checkpointing: Built-in support for logging and saving model checkpoints.\n",
        "- Flexibility: Allows for easy customization and extension to fit specific needs.\n",
        "\n",
        "Basic Concepts\n",
        "\n",
        "- LightningModule: The core class where you define your model, training, validation, and testing steps.\n",
        "- Trainer: Manages the training and validation process.\n",
        "- DataModule: Optional but recommended class to encapsulate data loading and preprocessing."
      ],
      "metadata": {
        "id": "lCe0rdo89h7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Code\n",
        "\n",
        "Here’s a simple example to illustrate how to use PyTorch Lightning.\n",
        "\n",
        "**1. Define the LightningModule**\n",
        "\n",
        "The LightningModule encapsulates your model definition and the training/validation steps."
      ],
      "metadata": {
        "id": "rqZmTpM29uDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(LitModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x.view(x.size(0), -1))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "        dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "1sZZ3W9o9yWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Training with Trainer**\n",
        "\n",
        "Create a Trainer object to manage the training process."
      ],
      "metadata": {
        "id": "05ZduiyN91QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "# Initialize the model\n",
        "model = LitModel()\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(max_epochs=5, gpus=1)  # Use gpus=1 for a single GPU or set to 0 for CPU\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(model)\n"
      ],
      "metadata": {
        "id": "YI9qWI0U-AeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Validation and Testing**\n",
        "\n",
        "Add validation and testing steps to the LightningModule if needed."
      ],
      "metadata": {
        "id": "BHzhlUMY-DfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "    # ... previous code ...\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        acc = torch.sum(torch.argmax(y_hat, dim=1) == y) / y.size(0)\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "        return {'val_loss': avg_loss, 'val_acc': avg_acc}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        acc = torch.sum(torch.argmax(y_hat, dim=1) == y) / y.size(0)\n",
        "        return {'test_loss': loss, 'test_acc': acc}\n"
      ],
      "metadata": {
        "id": "V8Gv9wDe-Gwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "trainer.test(model)"
      ],
      "metadata": {
        "id": "XyITXqTS-LV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Features**\n",
        "\n",
        "**1. DataModule**\n",
        "\n",
        "Encapsulates data loading and preprocessing logic."
      ],
      "metadata": {
        "id": "8g4aQN-w-P_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size=32):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "        self.train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "        self.val_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "63ovHsgq-PKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Callbacks**\n",
        "\n",
        "Custom actions during training, such as saving checkpoints or early stopping"
      ],
      "metadata": {
        "id": "nzFnP8KE-eBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_callback = ModelCheckpoint(monitor='val_loss', save_top_k=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Initialize the trainer with callbacks\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,\n",
        "    gpus=1,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
        ")"
      ],
      "metadata": {
        "id": "zrLvqkbn-hok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameter tuning with PyTorch**"
      ],
      "metadata": {
        "id": "oB15r9XV-tsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is a crucial step in developing machine learning models, as it involves finding the best combination of hyperparameters to improve model performance. With PyTorch, there are several approaches and libraries you can use for hyperparameter optimization. Here’s an overview of popular methods and a practical example using some of these approaches.\n",
        "\n",
        "**1. Manual Tuning**\n",
        "\n",
        "Manual tuning involves selecting hyperparameters based on intuition, experience, and trial-and-error. While this method is straightforward, it can be time-consuming and less efficient compared to automated approaches.\n",
        "\n",
        "**2. Grid Search**\n",
        "\n",
        "Grid Search is an exhaustive search over a specified hyperparameter grid. Although it is simple to implement, it can be computationally expensive and inefficient."
      ],
      "metadata": {
        "id": "-Xp44pvN-tyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example with scikit-learn's GridSearchCV (for models compatible with scikit-learn):**"
      ],
      "metadata": {
        "id": "uQqjsvFr-6GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a PyTorch model compatible with scikit-learn\n",
        "class PyTorchModel(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(PyTorchModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'hidden_units': [64, 128],\n",
        "    'lr': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "# Note: scikit-learn GridSearchCV is not directly compatible with PyTorch models\n"
      ],
      "metadata": {
        "id": "ka3msKDt-67d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For PyTorch models, you will need to use other methods like Optuna or Hyperopt."
      ],
      "metadata": {
        "id": "YXth_SWC_JYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Random Search**\n",
        "\n",
        "Random Search samples hyperparameters randomly from a specified range. It’s often more efficient than Grid Search, especially with large hyperparameter spaces.\n",
        "\n",
        "**4. Bayesian Optimization**\n",
        "\n",
        "Bayesian Optimization builds a probabilistic model to predict the performance of hyperparameters. It’s more efficient than random search and grid search for larger spaces.\n",
        "Example with Optuna:\n",
        "\n",
        "Optuna is a popular library for hyperparameter optimization that integrates well with PyTorch."
      ],
      "metadata": {
        "id": "lBuhgi7L_N3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class LitModel(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(LitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def objective(trial):\n",
        "    hidden_units = trial.suggest_int('hidden_units', 64, 128)\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "\n",
        "    # Load dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    model = LitModel(hidden_units)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(1):\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation (using the same dataset here for simplicity)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Optimize hyperparameters\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Best hyperparameters: \", study.best_params)\n",
        "print(\"Best accuracy: \", study.best_value)\n"
      ],
      "metadata": {
        "id": "6DhdXOfn_Sm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Hyperparameter Optimization Libraries**\n",
        "\n",
        "- Optuna: Provides a flexible and efficient way to perform hyperparameter optimization.\n",
        "- Hyperopt: Another library for optimization that supports Bayesian optimization.\n",
        "- Ray Tune: A scalable hyperparameter tuning library for distributed training.\n",
        "\n",
        "**6. Example with Hyperopt**\n",
        "\n",
        "Hyperopt is another popular library for hyperparameter tuning that uses Bayesian optimization."
      ],
      "metadata": {
        "id": "_iV8AFf4_ZjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LitModel(nn.Module):\n",
        "    def __init__(self, hidden_units):\n",
        "        super(LitModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def objective(params):\n",
        "    hidden_units = int(params['hidden_units'])\n",
        "    lr = params['lr']\n",
        "\n",
        "    # Load dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    model = LitModel(hidden_units)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(1):\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation (using the same dataset here for simplicity)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return -accuracy  # Hyperopt minimizes the objective function, so return negative accuracy\n",
        "\n",
        "# Define the search space\n",
        "space = {\n",
        "    'hidden_units': hp.choice('hidden_units', [64, 128]),\n",
        "    'lr': hp.loguniform('lr', -5, -1)  # Log-uniform distribution\n",
        "}\n",
        "\n",
        "# Optimize hyperparameters\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
        "\n",
        "print(\"Best hyperparameters: \", best)\n"
      ],
      "metadata": {
        "id": "4wYJqUOM_ika"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "- Manual Tuning: Simple but not efficient for large hyperparameter spaces.\n",
        "- Grid Search: Exhaustive but computationally expensive.\n",
        "- Random Search: More efficient than Grid Search.\n",
        "- Bayesian Optimization: More efficient and intelligent search using libraries like Optuna and Hyperopt.\n",
        "- Libraries: Optuna, Hyperopt, and Ray Tune provide advanced and efficient ways to perform hyperparameter tuning."
      ],
      "metadata": {
        "id": "nqcVXagg_mOh"
      }
    }
  ]
}