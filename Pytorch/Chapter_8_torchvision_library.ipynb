{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IQpgkg0Jv1UX",
        "CCdyGtlqwELF",
        "HpB1qUoawVjY",
        "HePolVHCwdFz",
        "_2fUM9Wlwo9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Datasets**\n",
        "\n",
        "**2. Transforms**\n",
        "\n",
        "**3. Models**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kSqZ30rHWzs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datasets**"
      ],
      "metadata": {
        "id": "IQpgkg0Jv1UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datasets**\n",
        "- Preloaded datasets:\n",
        "  - CIFAR10\n",
        "  - CIFAR100\n",
        "  - MNIST\n",
        "  - FashionMNIST\n",
        "  - KMNIST\n",
        "  - QMNIST\n",
        "  - EMNIST\n",
        "  - ImageNet\n",
        "  - Cityscapes\n",
        "  - COCO\n",
        "  - VOC\n",
        "  - STL10\n",
        "  - SBD\n",
        "  - SVHN\n",
        "  - USPS\n",
        "  - Flickr8k\n",
        "  - Flickr30k\n",
        "  - VOCSegmentation\n",
        "  - VOCDetection\n",
        "  - FakeData\n",
        "  - LSUN\n",
        "  - Places365\n",
        "  - Kinetics-400\n",
        "  - HMDB51\n",
        "  - UCF101\n",
        "  - Omniglot\n",
        "  - CelebA\n",
        "  - SBU"
      ],
      "metadata": {
        "id": "RW5JQSpJv3PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for training and testing sets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Iterate through the DataLoader and print batch shapes\n",
        "for images, labels in train_loader:\n",
        "    print('Train batch images shape:', images.shape)\n",
        "    print('Train batch labels shape:', labels.shape)\n",
        "    break\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    print('Test batch images shape:', images.shape)\n",
        "    print('Test batch labels shape:', labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "U87P0b7dw9jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Iterate through the DataLoader and print batch shapes\n",
        "for images, labels in train_loader:\n",
        "    print('Train batch images shape:', images.shape)\n",
        "    print('Train batch labels shape:', labels.shape)\n",
        "    break\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    print('Test batch images shape:', images.shape)\n",
        "    print('Test batch labels shape:', labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "u2l97neixged"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "root/dog/xxx.png\n",
        "\n",
        "root/dog/xxy.png\n",
        "\n",
        "root/dog/xxz.png\n",
        "\n",
        "root/cat/123.png\n",
        "\n",
        "root/cat/nsdf3.png\n",
        "\n",
        "root/cat/asd932_.png\n"
      ],
      "metadata": {
        "id": "0JpeAEQaxpdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load custom dataset using ImageFolder\n",
        "dataset = datasets.ImageFolder(root='./data/custom_dataset', transform=transform)\n",
        "\n",
        "# Create data loader\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Iterate through the DataLoader and print batch shapes\n",
        "for images, labels in data_loader:\n",
        "    print('Batch images shape:', images.shape)\n",
        "    print('Batch labels shape:', labels.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "1scG72ARxv2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transforms**"
      ],
      "metadata": {
        "id": "CCdyGtlqwELF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Transforms**\n",
        "- Image transformations:\n",
        "  - Compose\n",
        "  - ToTensor\n",
        "  - Normalize\n",
        "  - Resize\n",
        "  - CenterCrop\n",
        "  - RandomCrop\n",
        "  - RandomResizedCrop\n",
        "  - FiveCrop\n",
        "  - TenCrop\n",
        "  - RandomHorizontalFlip\n",
        "  - RandomVerticalFlip\n",
        "  - RandomRotation\n",
        "  - RandomAffine\n",
        "  - RandomPerspective\n",
        "  - RandomErasing\n",
        "  - ColorJitter\n",
        "  - Grayscale\n",
        "  - RandomGrayscale\n",
        "  - RandomAdjustSharpness\n",
        "  - RandomAutocontrast\n",
        "  - RandomInvert\n",
        "  - RandomPosterize\n",
        "  - RandomSolarize\n",
        "  - RandomEqualize\n",
        "  - RandomInvert\n",
        "  - RandomPosterize\n",
        "  - RandomSolarize\n",
        "  - RandomEqualize\n",
        "\n",
        "- Functional transforms:\n",
        "  - pad\n",
        "  - crop\n",
        "  - resize\n",
        "  - scale\n",
        "  - rotate\n",
        "  - adjust_brightness\n",
        "  - adjust_contrast\n",
        "  - adjust_gamma\n",
        "  - adjust_hue\n",
        "  - adjust_saturation\n",
        "  - erase\n",
        "  - perspective\n",
        "  - affine\n",
        "  - to_grayscale\n",
        "  - invert\n",
        "  - solarize\n",
        "  - posterize\n",
        "  - autocontrast\n",
        "  - equalize\n"
      ],
      "metadata": {
        "id": "Cbr7a0CnwIE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# Example of using functional transforms\n",
        "def custom_transform(image):\n",
        "    image = F.resize(image, size=128)\n",
        "    image = F.hflip(image)\n",
        "    image = F.to_tensor(image)\n",
        "    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    return image"
      ],
      "metadata": {
        "id": "JJ-cd2SlyPdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset with transformations\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for images, labels in dataloader:\n",
        "    print('Batch images shape:', images.shape)\n",
        "    print('Batch labels shape:', labels.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "rjXHb_lsySmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "HpB1qUoawVjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Models**\n",
        "- Pretrained models:\n",
        "  - AlexNet\n",
        "  - VGG\n",
        "  - ResNet\n",
        "  - SqueezeNet\n",
        "  - DenseNet\n",
        "  - Inception\n",
        "  - GoogleNet\n",
        "  - ShuffleNet\n",
        "  - MobileNetV2\n",
        "  - MobileNetV3\n",
        "  - ResNeXt\n",
        "  - WideResNet\n",
        "  - MnasNet\n",
        "  - EfficientNet\n",
        "  - RegNet\n",
        "  - VisionTransformer\n",
        "  - ConvNeXt\n",
        "  - SwinTransformer\n",
        "\n"
      ],
      "metadata": {
        "id": "n_eIfYYCwW8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load a pretrained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Load and preprocess an image\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image = Image.open(\"path_to_image.jpg\")\n",
        "input_tensor = preprocess(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "B2pIgz_fygos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load a pretrained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Replace the final fully connected layer to match the number of classes in your dataset\n",
        "num_classes = 10  # Example: 10 classes\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "\n",
        "# Define a simple transform for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "dataset = datasets.ImageFolder(root='./data/custom_dataset', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):  # Number of epochs\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "HC4YwdhZyzml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility functions**"
      ],
      "metadata": {
        "id": "HePolVHCwdFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utility Functions**\n",
        "- Image reading and writing:\n",
        "  - read_image\n",
        "  - write_png\n",
        "  - write_jpeg\n",
        "\n",
        "- Video reading:\n",
        "  - read_video\n",
        "  - read_video_timestamps\n",
        "  - write_video\n"
      ],
      "metadata": {
        "id": "o4ynEQ9BwlW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Read an image from file\n",
        "image_path = 'path_to_image.jpg'\n",
        "image_tensor = io.read_image(image_path)\n",
        "\n",
        "# Convert the tensor to a numpy array for visualization\n",
        "image_np = image_tensor.numpy().transpose(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image_np)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Print the tensor shape and data type\n",
        "print('Image tensor shape:', image_tensor.shape)\n",
        "print('Image tensor data type:', image_tensor.dtype)\n"
      ],
      "metadata": {
        "id": "9SsJZOSozyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read a video from file\n",
        "video_path = 'path_to_video.mp4'\n",
        "video_tensor, audio_tensor = io.read_video(video_path, start_pts=0, end_pts=10)  # Read the first 10 seconds of the video\n",
        "\n",
        "# Extract a single frame (e.g., the first frame)\n",
        "frame = video_tensor[0].numpy().transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
        "\n",
        "# Display the frame\n",
        "plt.imshow(frame)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Print tensor shapes\n",
        "print('Video tensor shape:', video_tensor.shape)\n",
        "print('Audio tensor shape:', audio_tensor.shape if audio_tensor is not None else 'No audio')\n"
      ],
      "metadata": {
        "id": "WJMW_hhvz70a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aditional functionalities**"
      ],
      "metadata": {
        "id": "_2fUM9Wlwo9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Additional Functionalities**\n",
        "- Dataset utilities:\n",
        "  - ImageFolder\n",
        "  - DatasetFolder\n",
        "  - DataLoader\n",
        "  - ConcatDataset\n",
        "  - Subset\n",
        "  - random_split\n",
        "\n",
        "- Transforms utilities:\n",
        "  - Lambda\n",
        "  - RandomApply\n",
        "  - RandomOrder\n",
        "  - LinearTransformation\n",
        "  - RandomChoice"
      ],
      "metadata": {
        "id": "pPtAd5Q8wv4U"
      }
    }
  ]
}