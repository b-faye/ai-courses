{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Introduction to neural networks**\n",
        "    \n",
        "**2. Creating a simple neural network**\n",
        "    \n",
        "**3. Activation functions**\n",
        "    \n",
        "**4. Loss functions and optimization**\n",
        "\n"
      ],
      "metadata": {
        "id": "RxIdA3ramh0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to neural networks**"
      ],
      "metadata": {
        "id": "cEBM4gU2wmqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks, inspired by the human brain, are a fundamental concept in machine learning, particularly in deep learning. They are used to model complex patterns and make predictions based on data. PyTorch, an open-source machine learning library developed by Facebook's AI Research lab, provides an intuitive and flexible way to build and train neural networks. This introduction covers the basics of neural networks and how to implement them using PyTorch.\n",
        "\n",
        "\n",
        "\n",
        "**Key components:**\n",
        "\n",
        "A neural network consists of layers of interconnected nodes (neurons). Each neuron receives inputs, processes them, and passes the result to the next layer. The most basic type of neural network is the feedforward neural network, where connections do not form cycles.\n",
        "\n",
        "- **Neurons:** Basic units of a neural network.\n",
        "- **Layers:** Combinations of neurons; typically include input, hidden, and output layers.\n",
        "- **Weights and Biases:** Parameters that the network learns during training.\n",
        "- **Activation Function:** Applies a non-linear transformation to the input, enabling the network to learn complex patterns.\n",
        "\n",
        "**Common activation functions:**\n",
        "- **Sigmoid:** $\\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
        "- **ReLU (Rectified Linear Unit):** $ \\text{ReLU}(x) = \\max(0, x) $\n",
        "- **Tanh:** $ \\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n"
      ],
      "metadata": {
        "id": "ZAdN0F6PxJpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a simple neural network**"
      ],
      "metadata": {
        "id": "9Imt-v4jxhjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a simple neural network with PyTorch involves several steps: setting up the environment, defining the network architecture, preparing the data, and training and evaluating the model. Below, I'll walk you through the entire process with a simple example."
      ],
      "metadata": {
        "id": "O7gqxDDJzLqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup**\n",
        "\n",
        "First, make sure you have PyTorch installed. If not, you can install it using:"
      ],
      "metadata": {
        "id": "WlgU_11pzM1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch torchvision"
      ],
      "metadata": {
        "id": "wEtCMBTCzSv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Import Libraries**"
      ],
      "metadata": {
        "id": "dESxAhVHzViU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "r9Q6xacNzaqg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Define the Neural Network**\n",
        "\n",
        "Here, we define a simple feedforward neural network with one hidden layer:"
      ],
      "metadata": {
        "id": "Ww5wnx0Lzcdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # 784 input features (28x28 pixels), 128 hidden units\n",
        "        self.fc2 = nn.Linear(128, 10)       # 128 hidden units, 10 output classes (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input tensor\n",
        "        x = F.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)          # Output layer\n",
        "        return F.log_softmax(x, dim=1)  # Apply log-softmax"
      ],
      "metadata": {
        "id": "0TO_KnXVzjWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Prepare the Data**\n",
        "\n",
        "Load and preprocess the MNIST dataset using torchvision:"
      ],
      "metadata": {
        "id": "KkxMVzqHznuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST('.', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
      ],
      "metadata": {
        "id": "F0BpcMZXzr-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LCZzXb90B8o",
        "outputId": "25438fb5-9942-4f81-866c-fb4f0f611d16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: .\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, target in train_loader:\n",
        "  break"
      ],
      "metadata": {
        "id": "8smYHdqG0KXT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t00klGbj0VVp",
        "outputId": "5c863969-8b4a-4432-931f-bc52ad5d1ed0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MIHbN2V0YiR",
        "outputId": "b50132d6-5468-4432-f5c2-4447cd63efa5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "midHOS-n2g9Q",
        "outputId": "af67f3b3-760d-4089-9262-7e02e5ca92f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 9, 0, 0, 1, 1, 7, 9, 1, 9, 0, 9, 1, 9, 1, 0, 0, 7, 7, 9, 1, 6, 8, 9,\n",
              "        0, 0, 7, 0, 6, 0, 6, 3, 3, 5, 8, 2, 3, 7, 7, 4, 2, 1, 4, 1, 4, 1, 8, 0,\n",
              "        2, 5, 8, 6, 4, 7, 2, 7, 1, 1, 3, 9, 2, 0, 5, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Initialize the Model, Loss Function, and Optimizer**"
      ],
      "metadata": {
        "id": "Wy-b9wKM0kJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNN()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "J7sSyJmW0lwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Train the Model**\n",
        "\n",
        "Define the training loop:"
      ],
      "metadata": {
        "id": "P3e3Opm50pRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
      ],
      "metadata": {
        "id": "62X0en6X03Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Test the Model**\n",
        "\n",
        "Define the testing loop:"
      ],
      "metadata": {
        "id": "bW9LAr-Y1Eal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # Sum batch losses\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
      ],
      "metadata": {
        "id": "Epvzi-6v1KiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8: Run Training and Testing**\n",
        "\n",
        "Train and test the model for a specified number of epochs:"
      ],
      "metadata": {
        "id": "FarHy5Q-1QUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 11):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test(model, test_loader)"
      ],
      "metadata": {
        "id": "dgM0HSzv1S_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation functions**"
      ],
      "metadata": {
        "id": "fJf-A1UB1bky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, activation functions are available in the torch.nn and torch.nn.functional modules. These functions introduce non-linearity into the model, enabling it to learn complex patterns. Here are some commonly used activation functions and how to use them in PyTorch:"
      ],
      "metadata": {
        "id": "6IQPdOfP2mvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Sigmoid**\n",
        "\n",
        "The sigmoid function maps the input to a value between 0 and 1. It is often used in binary classification problems."
      ],
      "metadata": {
        "id": "1GUukN932nbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.tensor([-1.0, 0.0, 1.0])\n",
        "y = torch.sigmoid(x)\n",
        "# or\n",
        "y = F.sigmoid(x)"
      ],
      "metadata": {
        "id": "6CQ_Iubc2stm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tanh (Hyperbolic Tangent)**\n",
        "\n",
        "The tanh function maps the input to a value between -1 and 1. It is zero-centered, making it an improvement over the sigmoid function."
      ],
      "metadata": {
        "id": "Plv-E-Vy2yBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tanh(x)\n",
        "# or\n",
        "y = F.tanh(x)"
      ],
      "metadata": {
        "id": "EKADMG8-25Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. ReLU (Rectified Linear Unit)**\n",
        "\n",
        "The ReLU function is the most commonly used activation function in deep learning models. It outputs zero if the input is negative; otherwise, it outputs the input."
      ],
      "metadata": {
        "id": "3mT4I78m27_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.relu(x)\n",
        "# or\n",
        "y = F.relu(x)"
      ],
      "metadata": {
        "id": "uOYcbt963Aj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Leaky ReLU**\n",
        "\n",
        "The Leaky ReLU function allows a small, non-zero gradient when the input is negative, which helps mitigate the dying ReLU problem."
      ],
      "metadata": {
        "id": "o_uDKVWi3E_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "y = leaky_relu(x)\n",
        "# or\n",
        "y = F.leaky_relu(x, negative_slope=0.01)"
      ],
      "metadata": {
        "id": "IB2kbZNs3KTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. ELU (Exponential Linear Unit)**\n",
        "\n",
        "The ELU function helps with the vanishing gradient problem by allowing the outputs to be negative and have non-zero gradients."
      ],
      "metadata": {
        "id": "ZzORAfFc3MJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elu = nn.ELU(alpha=1.0)\n",
        "y = elu(x)\n",
        "# or\n",
        "y = F.elu(x, alpha=1.0)"
      ],
      "metadata": {
        "id": "8DFrv0FP3QK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Softmax**\n",
        "\n",
        "The Softmax function is often used in the output layer of a classification network to represent probabilities. It outputs a vector that sums to 1."
      ],
      "metadata": {
        "id": "sRDiVsmu3XBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.softmax(x, dim=1)\n",
        "# or\n",
        "y = F.softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "ieGEPzzh3Y7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Log Softmax**\n",
        "\n",
        "The Log Softmax function applies the logarithm to the Softmax output, which is useful for numerical stability in conjunction with the negative log-likelihood loss."
      ],
      "metadata": {
        "id": "IxEQRWlc3bCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.log_softmax(x, dim=1)\n",
        "# or\n",
        "y = F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "CZWmY_SJ3gQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Using Activation Functions in a Neural Network**\n",
        "\n",
        "Here's an example of a simple neural network using different activation functions:"
      ],
      "metadata": {
        "id": "Rv4aLAd73sg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(128, 64)       # Hidden layer to another hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)        # Hidden layer to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input tensor\n",
        "\n",
        "        # Using ReLU activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Using Leaky ReLU activation function\n",
        "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
        "\n",
        "        # Using Log Softmax activation function for the output layer\n",
        "        x = F.log_softmax(self.fc3(x), dim=1)\n",
        "        return x\n",
        "\n",
        "# Create a model instance\n",
        "model = SimpleNN()\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "id": "1O9ahIRD3uU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- `self.fc1`, `self.fc2`, `self.fc3`: Define fully connected layers.\n",
        "- `F.relu(self.fc1(x))`: Apply the ReLU activation function after the first fully connected layer.\n",
        "- `F.leaky_relu(self.fc2(x), negative_slope=0.01)`: Apply the Leaky ReLU activation function after the second fully connected layer.\n",
        "- `F.log_softmax(self.fc3(x), dim=1)`: Apply the Log Softmax activation function to the output layer, which is typical for classification tasks.\n",
        "\n",
        "These activation functions can be mixed and matched within the network to best suit the problem you're tackling. Experimenting with different activation functions and their placements within your network can significantly affect the performance of your model."
      ],
      "metadata": {
        "id": "8eJ6Y0xb31It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss functions**"
      ],
      "metadata": {
        "id": "h-DXmL9I3_hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions, also known as cost functions or objective functions, measure the difference between the predicted output of a neural network and the actual target values. In PyTorch, loss functions are available in the torch.nn module. They are crucial for training neural networks, as they guide the optimization process by providing a measure of how well the network is performing."
      ],
      "metadata": {
        "id": "RteR6vv-4cw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Mean Squared Error Loss (MSELoss)**\n",
        "\n",
        "Used for regression tasks, it calculates the average of the squared differences between predicted and actual values."
      ],
      "metadata": {
        "id": "NmVZnxpO4dnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "predicted = torch.tensor([0.5, 0.8], requires_grad=True)\n",
        "target = torch.tensor([0.3, 1.0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbLCndjP4nPg",
        "outputId": "e35b0da7-465c-4487-b66b-93fc813a3902"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0400, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCuZH2UE4rMf",
        "outputId": "d943f799-3cdb-4393-b937-f343aaa54a8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03999999538064003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Binary Cross-Entropy Loss (BCELoss)**\n",
        "\n",
        "Used for binary classification tasks, it measures the binary cross-entropy loss between the predicted and actual values."
      ],
      "metadata": {
        "id": "sTM0Byqq5iI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "predicted = torch.tensor([0.5, 0.8], requires_grad=True)\n",
        "target = torch.tensor([0.0, 1.0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FakoY4NA5mHU",
        "outputId": "bae48396-94b8-41e3-c433-0a24181bc9f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4581, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss)**\n",
        "\n",
        "Combines a Sigmoid layer and the BCELoss in one single class. This is more numerically stable than using a plain Sigmoid followed by a BCELoss (**no need to define explicitly sigmoid activation in the classification layer**)."
      ],
      "metadata": {
        "id": "VgXhI1a66Yyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "predicted = torch.tensor([0.5, 0.8], requires_grad=True)\n",
        "target = torch.tensor([0.0, 1.0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av8s_4JK6cF3",
        "outputId": "ea68e34c-f112-46ae-e40d-71bf65e609ef"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6726, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Cross-Entropy Loss (CrossEntropyLoss)**\n",
        "\n",
        "Used for multi-class classification tasks. It combines LogSoftmax and NLLLoss in one single class (**no need to define explicitly LogSoftmax activation in the classification layer**)."
      ],
      "metadata": {
        "id": "0cDPZUo56gp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "predicted = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True)\n",
        "target = torch.tensor([0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaoJao9C6ixU",
        "outputId": "9a205f4a-b7c5-41f2-ab70-188b2e58e733"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4170, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Negative Log-Likelihood Loss (NLLLoss)**\n",
        "\n",
        "Used for classification tasks with LogSoftmax output."
      ],
      "metadata": {
        "id": "T2hTwCHw7PiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.NLLLoss()\n",
        "predicted = torch.tensor([[2.0, 1.0, 0.1]], requires_grad=True).log_softmax(dim=1)\n",
        "target = torch.tensor([0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq7Lj_ny7Uwd",
        "outputId": "87aa46fc-fed3-4936-8990-10acf0c1ab83"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4170, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Smooth L1 Loss**\n",
        "\n",
        "Combines the benefits of L1 and L2 loss. It is less sensitive to outliers than MSELoss."
      ],
      "metadata": {
        "id": "D02yHy537u0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.SmoothL1Loss()\n",
        "predicted = torch.tensor([0.5, 0.8], requires_grad=True)\n",
        "target = torch.tensor([0.3, 1.0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "iwDPMl_D7yVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Huber Loss**\n",
        "\n",
        "Another loss function that is less sensitive to outliers in data than the squared error loss."
      ],
      "metadata": {
        "id": "jMuDb1WQ74ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.HuberLoss()\n",
        "predicted = torch.tensor([0.5, 0.8], requires_grad=True)\n",
        "target = torch.tensor([0.3, 1.0])\n",
        "loss = loss_fn(predicted, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "ohDYX4Di76Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Using Loss Functions in a Neural Network**\n",
        "\n",
        "Hereâ€™s an example of a neural network for a classification task using CrossEntropyLoss:"
      ],
      "metadata": {
        "id": "32GaBMLD8Diq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "def train(model, train_loader, loss_fn, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    train(model, train_loader, loss_fn, optimizer, epoch)"
      ],
      "metadata": {
        "id": "yhEU7WV78HpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "- **Model Definition:** A simple feedforward neural network with two hidden layers and ReLU activations.\n",
        "- **Data Preparation:** MNIST dataset with normalization.\n",
        "- **Loss Function:** CrossEntropyLoss is used for the classification task.\n",
        "- **Training Loop:** Trains the model using the training dataset, computes the loss, performs backpropagation, and updates the model parameters using the optimizer.\n",
        "\n",
        "By using the appropriate loss function for your task, you can ensure that your neural network is trained effectively to minimize the error and improve its performance on the given problem.\n"
      ],
      "metadata": {
        "id": "cH4wtBZL8MXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimization functions**"
      ],
      "metadata": {
        "id": "UZq0kQdG8rHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimization is a crucial part of training neural networks, as it involves adjusting the model parameters to minimize the loss function. PyTorch provides various optimization algorithms through the `torch.optim` module. Here's a guide to using optimizers in PyTorch.\n",
        "\n",
        "1. **Stochastic Gradient Descent (SGD)**\n",
        "   The simplest optimization algorithm that updates the parameters using the gradient of the loss function.\n",
        "   ```python\n",
        "   optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "   ```\n",
        "\n",
        "2. **SGD with Momentum**\n",
        "   Improves upon SGD by adding a momentum term to help accelerate gradients vectors in the right directions.\n",
        "   ```python\n",
        "   optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "   ```\n",
        "\n",
        "3. **Adam (Adaptive Moment Estimation)**\n",
        "   Combines the advantages of two other extensions of SGD, AdaGrad and RMSProp. It works well on large datasets and is computationally efficient.\n",
        "   ```python\n",
        "   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "   ```\n",
        "\n",
        "4. **RMSprop**\n",
        "   An adaptive learning rate method that divides the learning rate by an exponentially decaying average of squared gradients.\n",
        "   ```python\n",
        "   optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "   ```\n",
        "\n",
        "5. **Adagrad (Adaptive Gradient Algorithm)**\n",
        "   Adaptively scales the learning rate for each parameter based on the historical gradient information.\n",
        "   ```python\n",
        "   optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
        "   ```\n",
        "\n",
        "6. **Adadelta**\n",
        "   An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\n",
        "   ```python\n",
        "   optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0)\n",
        "   ```\n",
        "\n",
        "7. **AdamW**\n",
        "   Similar to Adam, but with correct weight decay, which helps prevent overfitting.\n",
        "   ```python\n",
        "   optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "   ```\n"
      ],
      "metadata": {
        "id": "-nkShhfe8u6E"
      }
    }
  ]
}