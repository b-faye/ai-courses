{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Understanding convolutional layers**\n",
        "    \n",
        "**2. Pooling layers**\n",
        "    \n",
        "**3. Building a simple CNN**\n",
        "    \n",
        "**4. Transfer learning with pre-trained models**\n",
        "\n"
      ],
      "metadata": {
        "id": "RxIdA3ramh0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding convolutional layers**"
      ],
      "metadata": {
        "id": "c6EfSToGRYAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding convolutional layers in PyTorch involves grasping both the theoretical concepts behind convolutional neural networks (CNNs) and how to implement these layers using PyTorch's **torch.nn** module. Here's a breakdown of the key concepts and a simple example to illustrate their implementation."
      ],
      "metadata": {
        "id": "CY7vvCuNYF_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Layer Parameters**\n",
        "\n",
        "1. **Stride**:\n",
        "   - Controls the step size of the filter when it moves across the input image.\n",
        "   - Can be a single integer or a tuple of two integers (height and width stride).\n",
        "   - Example: `stride=2` or `stride=(2, 2)`.\n",
        "\n",
        "2. **Padding**:\n",
        "   - Adds a border of zeros around the input image to control the spatial dimensions of the output.\n",
        "   - Can be an integer, a tuple, or a string (`'valid'` or `'same'`).\n",
        "   - `'valid'` means no padding, and `'same'` means padding is added so that the output dimensions are the same as the input dimensions.\n",
        "   - Example: `padding=1`, `padding=(1, 1)`, `padding='same'`.\n",
        "\n",
        "3. **Dilation**:\n",
        "   - Controls the spacing between kernel points.\n",
        "   - Can be a single integer or a tuple of two integers.\n",
        "   - Increases the receptive field of the convolutional kernel.\n",
        "   - Example: `dilation=2` or `dilation=(2, 2)`.\n",
        "\n",
        "4. **Groups**:\n",
        "   - Controls the connections between inputs and outputs.\n",
        "   - `groups=1` means all inputs are convolved to all outputs.\n",
        "   - `groups=2` means the input channels are split into two groups, each convolved separately.\n",
        "   - `groups=in_channels` means a depthwise convolution where each input channel is convolved with its own set of filters.\n",
        "   - Example: `groups=1`, `groups=2`, `groups=in_channels`.\n",
        "\n",
        "**Output Shape Formula**\n",
        "\n",
        "The output shape of a convolutional layer can be calculated using the following formula:\n",
        "\n",
        "For each dimension $ i $ (height or width):\n",
        "$$ \\text{Output size}_i = \\left\\lfloor \\frac{\\text{Input size}_i + 2 \\times \\text{Padding}_i - \\text{Dilation}_i \\times (\\text{Kernel size}_i - 1) - 1}{\\text{Stride}_i} + 1 \\right\\rfloor $$\n"
      ],
      "metadata": {
        "id": "kaK8MgTdZFJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1: Basic Convolution**"
      ],
      "metadata": {
        "id": "4zdrwTtSaayi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Basic convolution with 1 input channel, 6 output channels, and 3x3 kernel\n",
        "conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n",
        "input_tensor1 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor1 = conv1(input_tensor1)\n",
        "print(f\"Example 1 Output Shape: {output_tensor1.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX1C707qZ0Gy",
        "outputId": "846eda4c-3e3b-4184-a24d-3a647885d105"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 Output Shape: torch.Size([1, 6, 26, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 1:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 0 - 1 \\times (3 - 1) - 1}{1} + 1 \\right\\rfloor = 26 $$\n",
        "So the output shape is `(1, 6, 26, 26)`.\n"
      ],
      "metadata": {
        "id": "WP01t9H2a0RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2: Convolution with Stride**"
      ],
      "metadata": {
        "id": "Uczc0_VIb5NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 1 input channel, 6 output channels, 3x3 kernel, and stride of 2\n",
        "conv2 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=2)\n",
        "input_tensor2 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor2 = conv2(input_tensor2)\n",
        "print(f\"Example 2 Output Shape: {output_tensor2.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwR-MCZHb6x2",
        "outputId": "a757a601-78f1-44da-d9f1-03edb077fe7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 2 Output Shape: torch.Size([1, 6, 13, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 2:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 0 - 1 \\times (3 - 1) - 1}{2} + 1 \\right\\rfloor = 13 $$\n",
        "So the output shape is `(1, 6, 13, 13)`.\n"
      ],
      "metadata": {
        "id": "Lqq60SWyb_fW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3: Convolution with Padding**"
      ],
      "metadata": {
        "id": "7-iHu-iRcSdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 1 input channel, 6 output channels, 3x3 kernel, and padding of 1\n",
        "conv3 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, padding=1)\n",
        "input_tensor3 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor3 = conv3(input_tensor3)\n",
        "print(f\"Example 3 Output Shape: {output_tensor3.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvzCqbUbcWED",
        "outputId": "60251996-61fd-48c0-e381-a336ba51d334"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 3 Output Shape: torch.Size([1, 6, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 3:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 1 - 1 \\times (3 - 1) - 1}{1} + 1 \\right\\rfloor = 28 $$\n",
        "So the output shape is `(1, 6, 28, 28)`.\n",
        "\n"
      ],
      "metadata": {
        "id": "LKG7SPBmcgHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4: Convolution with Dilation**"
      ],
      "metadata": {
        "id": "y2Wz-tTcdTtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 1 input channel, 6 output channels, 3x3 kernel, and dilation of 2\n",
        "conv4 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, dilation=2)\n",
        "input_tensor4 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor4 = conv4(input_tensor4)\n",
        "print(f\"Example 4 Output Shape: {output_tensor4.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSa49w94dVl3",
        "outputId": "ba4f7c22-4e04-4921-e989-a8dfd32aa23e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 4 Output Shape: torch.Size([1, 6, 24, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 4:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 0 - 2 \\times (3 - 1) - 1}{1} + 1 \\right\\rfloor = 24 $$\n",
        "So the output shape is `(1, 6, 24, 24)`.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYoFlTlidZns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 5: Convolution with Different Kernel Sizes**"
      ],
      "metadata": {
        "id": "wj4UvyMTdmX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 1 input channel, 6 output channels, 5x5 kernel\n",
        "conv5 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "input_tensor5 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor5 = conv5(input_tensor5)\n",
        "print(f\"Example 5 Output Shape: {output_tensor5.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v10rDHAEdrOF",
        "outputId": "02b8b0b4-e964-4ced-be74-bf32cba8ba79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 5 Output Shape: torch.Size([1, 6, 24, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 5:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 0 - 1 \\times (5 - 1) - 1}{1} + 1 \\right\\rfloor = 24 $$\n",
        "So the output shape is `(1, 6, 24, 24)`."
      ],
      "metadata": {
        "id": "O_K2ikmrduI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 6: Convolution with Multiple Input Channels**"
      ],
      "metadata": {
        "id": "BDQWj2eBd5ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 3 input channels (e.g., RGB image), 6 output channels, and 3x3 kernel\n",
        "conv6 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3)\n",
        "input_tensor6 = torch.randn(1, 3, 28, 28)  # Batch size of 1, 3 channels (RGB), 28x28 image\n",
        "output_tensor6 = conv6(input_tensor6)\n",
        "print(f\"Example 6 Output Shape: {output_tensor6.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sPp4sNSeGV9",
        "outputId": "3abc18c5-c0b4-4046-a8a7-f07c8994bf1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 6 Output Shape: torch.Size([1, 6, 26, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 6:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 0 - 1 \\times (3 - 1) - 1}{1} + 1 \\right\\rfloor = 26 $$\n",
        "So the output shape is `(1, 6, 26, 26)`.\n"
      ],
      "metadata": {
        "id": "ExuaBDSIeLaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 7: Convolution with Grouping**"
      ],
      "metadata": {
        "id": "RK4UC_AqeX4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 4 input channels, 8 output channels, 3x3 kernel, and groups of 2\n",
        "conv7 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, groups=2)\n",
        "input_tensor7 = torch.randn(1, 4, 28, 28)  # Batch size of 1, 4 channels, 28x28 image\n",
        "output_tensor7 = conv7(input_tensor7)\n",
        "print(f\"Example 7 Output Shape: {output_tensor7.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-i9J9Aoeb9C",
        "outputId": "4c01c307-b143-41dc-b53c-737219596d75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 7 Output Shape: torch.Size([1, 8, 26, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 7:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 0 - 1 \\times (3 - 1) - 1}{1} + 1 \\right\\rfloor = 26 $$\n",
        "So the output shape is `(1, 8, 26, 26)`.\n",
        "\n"
      ],
      "metadata": {
        "id": "0mVFmnI8ewfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 8: Convolution with Different Stride and Padding**"
      ],
      "metadata": {
        "id": "lockKk4ve90o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution with 1 input channel, 6 output channels, 3x3 kernel, stride of 2, and padding of 1\n",
        "conv8 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=2, padding=1)\n",
        "input_tensor8 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor8 = conv8(input_tensor8)\n",
        "print(f\"Example 8 Output Shape: {output_tensor8.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXhlyWTTfA1l",
        "outputId": "26be5ade-b2f5-4e1a-9d72-5c03705232d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 8 Output Shape: torch.Size([1, 6, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output shape formula for Example 8:\n",
        "$$ \\text{Output size} = \\left\\lfloor \\frac{28 + 2 \\times 1 - 1 \\times (3 - 1) - 1}{2} + 1 \\right\\rfloor = 14 $$\n",
        "So the output shape is `(1, 6, 14, 14)`.\n"
      ],
      "metadata": {
        "id": "GlSZUAnhfUeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pooling layers**"
      ],
      "metadata": {
        "id": "54T9t243fjnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pooling layers in PyTorch are used to reduce the spatial dimensions of the input tensor, effectively downsampling the input. This is done to reduce the computational load, control overfitting, and allow the network to learn spatial hierarchies. The most common types of pooling layers are Max Pooling and Average Pooling. Below, we'll cover these types and provide examples with varying parameters.\n",
        "\n",
        "**Max Pooling** and **Average Pooling**\n",
        "\n",
        "Max Pooling takes the maximum value from each patch of the feature map.\n",
        "\n",
        "Average Pooling calculates the average value from each patch of the feature map.\n",
        "\n",
        "**Parameters:**\n",
        "- **kernel_size**: The size of the window to take a max over.\n",
        "- **stride**: The stride of the window. Default value is kernel_size.\n",
        "- **padding**: Implicit zero padding to be added on both sides.\n",
        "- **dillation:**\n",
        "\n",
        "For each dimension $ i $ (height or width): by default stride = 2\n",
        "$$ \\text{Output size}_i = \\left\\lfloor \\frac{\\text{Input size}_i + 2 \\times \\text{Padding}_i - \\text{Dilation}_i \\times (\\text{Kernel size}_i - 1) - 1}{\\text{Stride}_i} + 1 \\right\\rfloor $$"
      ],
      "metadata": {
        "id": "2lpKwKgojI8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Max Pooling</h2>**"
      ],
      "metadata": {
        "id": "zkgZZDFEjseO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1: Basic Max Pooling**"
      ],
      "metadata": {
        "id": "uPg4OkYUkWho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Basic max pooling with 2x2 kernel\n",
        "max_pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "input_tensor1 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor1 = max_pool1(input_tensor1)\n",
        "print(f\"Example 1 Output Shape: {output_tensor1.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J-qn0e4kZlf",
        "outputId": "17762f6d-0d01-4ab9-acb1-0d34ddc97484"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 Output Shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2: Max Pooling with Stride**"
      ],
      "metadata": {
        "id": "wOpNqHcHkleh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max pooling with 2x2 kernel and stride of 2\n",
        "max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "input_tensor2 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor2 = max_pool2(input_tensor2)\n",
        "print(f\"Example 2 Output Shape: {output_tensor2.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1be3QoR2km4t",
        "outputId": "c993c874-c3b4-490c-96ca-d2f4cde98e12"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 2 Output Shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3: Max Pooling with Padding**"
      ],
      "metadata": {
        "id": "Nx1DSyg-kpg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max pooling with 3x3 kernel, stride of 2, and padding of 1\n",
        "max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "input_tensor3 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor3 = max_pool3(input_tensor3)\n",
        "print(f\"Example 3 Output Shape: {output_tensor3.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU9XzggxkukP",
        "outputId": "16570d4b-c319-4f8f-d91f-b918052e6963"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 3 Output Shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Average Pooling</h2>**"
      ],
      "metadata": {
        "id": "o5FIOINMmHbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4: Basic Average Pooling**"
      ],
      "metadata": {
        "id": "PQrhQaYUmOGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic average pooling with 2x2 kernel\n",
        "avg_pool1 = nn.AvgPool2d(kernel_size=2)\n",
        "input_tensor4 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor4 = avg_pool1(input_tensor4)\n",
        "print(f\"Example 4 Output Shape: {output_tensor4.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9nnSobImL31",
        "outputId": "2fd75ff5-a249-46b3-a76c-f7d80b40aa93"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 4 Output Shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 5: Average Pooling with Stride**"
      ],
      "metadata": {
        "id": "8f7jgTqhmcgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average pooling with 2x2 kernel and stride of 2\n",
        "avg_pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "input_tensor5 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor5 = avg_pool2(input_tensor5)\n",
        "print(f\"Example 5 Output Shape: {output_tensor5.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARo5Uc6Lmftz",
        "outputId": "e0f42f69-47e0-4c13-8443-c9b607e6f3ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 5 Output Shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 6: Average Pooling with Padding**"
      ],
      "metadata": {
        "id": "D8pcIJTRmj5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average pooling with 3x3 kernel, stride of 2, and padding of 1\n",
        "avg_pool3 = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
        "input_tensor6 = torch.randn(1, 1, 28, 28)  # Batch size of 1, 1 channel, 28x28 image\n",
        "output_tensor6 = avg_pool3(input_tensor6)\n",
        "print(f\"Example 6 Output Shape: {output_tensor6.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdwcBfjummmY",
        "outputId": "0ed8df34-540f-4b03-ad73-09a52bca7375"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 6 Output Shape: torch.Size([1, 1, 14, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building a simple CNN**"
      ],
      "metadata": {
        "id": "UwQbZ4Mbmxrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PWqR-QLDpLna"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Define the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Define a fully connected layer\n",
        "        self.fc1 = nn.Linear(in_features=32 * 7 * 7, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first convolutional layer, followed by ReLU activation and max pooling\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "\n",
        "        # Apply the second convolutional layer, followed by ReLU activation and max pooling\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten the tensor into a vector\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "\n",
        "        # Apply the first fully connected layer with ReLU activation\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Apply the second fully connected layer (output layer)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Hzd86P1apOkX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the training and test sets\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Download and load the training and test datasets\n",
        "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "s3XqPBQ7paKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)"
      ],
      "metadata": {
        "id": "qvGsTCEmpcj4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "TLio3v0Yux6o",
        "outputId": "5fba945b-2b1c-4888-a533-7817c76cbb17"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of Linear(in_features=1568, out_features=128, bias=True)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.module.Module.parameters</b><br/>def parameters(recurse: bool=True) -&gt; Iterator[Parameter]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</a>Return an iterator over module parameters.\n",
              "\n",
              "This is typically passed to an optimizer.\n",
              "\n",
              "Args:\n",
              "    recurse (bool): if True, then yields parameters of this module\n",
              "        and all submodules. Otherwise, yields only parameters that\n",
              "        are direct members of this module.\n",
              "\n",
              "Yields:\n",
              "    Parameter: module parameter\n",
              "\n",
              "Example::\n",
              "\n",
              "    &gt;&gt;&gt; # xdoctest: +SKIP(&quot;undefined vars&quot;)\n",
              "    &gt;&gt;&gt; for param in model.parameters():\n",
              "    &gt;&gt;&gt;     print(type(param), param.size())\n",
              "    &lt;class &#x27;torch.Tensor&#x27;&gt; (20L,)\n",
              "    &lt;class &#x27;torch.Tensor&#x27;&gt; (20L, 1L, 5L, 5L)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2207);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "d481AY7Mpj3g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader, total=len(train_loader)):\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images.to(device))\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Train the model for 5 epochs\n",
        "train(model, train_loader, criterion, optimizer, epochs=5)\n"
      ],
      "metadata": {
        "id": "OmJ43mbxqCfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, total=len(test_loader)):\n",
        "            outputs = model(images.to(device))\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluate(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R90gk9pdrb1i",
        "outputId": "dc147805-e1cd-4442-919c-b4f138385138"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 29.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transfer learning with pre-trained models**"
      ],
      "metadata": {
        "id": "imHVxahdsWfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a powerful technique in deep learning where a model developed for one task is reused as the starting point for a model on a second task. Using pre-trained models can significantly speed up the training process and improve performance, especially when dealing with limited data. PyTorch provides several pre-trained models through the torchvision.models module.\n",
        "\n",
        "Here is a step-by-step guide to implementing transfer learning using a pre-trained model, such as ResNet, to classify images from a different dataset, such as CIFAR-10."
      ],
      "metadata": {
        "id": "ItIytvPNsvO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "iH6OfOwltAZ7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the training and test sets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize images to 224x224 as expected by ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Download and load the training and test datasets\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "wqQE-QUOtalK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze the parameters of the pre-trained layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the final fully connected layer to match the number of output classes (CIFAR-10 has 10 classes)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)  # CIFAR-10 has 10 output classes"
      ],
      "metadata": {
        "id": "JNc15ca8teWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # Only train the final layer"
      ],
      "metadata": {
        "id": "74oAxPY7t_xF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize the model\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Train the model for 5 epochs\n",
        "train(model, train_loader, criterion, optimizer, epochs=5)"
      ],
      "metadata": {
        "id": "Hs1qtWIQuLJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)  # Forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluate(model, test_loader)"
      ],
      "metadata": {
        "id": "nN5_TSGuuN1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}