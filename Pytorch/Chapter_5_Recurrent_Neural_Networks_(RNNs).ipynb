{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Basics of sequential data**\n",
        "\n",
        "**2. Introduction to RNNs**\n",
        "\n",
        "**3. Long Short-Term Memory (LSTM) networks**\n",
        "\n",
        "**4. Gated Recurrent Unit (GRU)**\n",
        "\n"
      ],
      "metadata": {
        "id": "RxIdA3ramh0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basics of sequential data**"
      ],
      "metadata": {
        "id": "4swHLqZazRLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential data is a type of data where the order of the elements is significant. In PyTorch, handling sequential data typically involves the use of recurrent neural networks (RNNs), Long Short-Term Memory networks (LSTMs), and Gated Recurrent Units (GRUs). Here are some key concepts and basic steps for working with sequential data in PyTorch:\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "1. **Recurrent Neural Networks (RNNs)**:\n",
        "    - RNNs are designed to handle sequential data by maintaining a hidden state that captures information about previous elements in the sequence.\n",
        "    - Basic structure: `hidden_state_t = f(hidden_state_t-1, input_t)`\n",
        "\n",
        "2. **Long Short-Term Memory Networks (LSTMs)**:\n",
        "    - LSTMs address the vanishing gradient problem in RNNs by introducing memory cells and gates (input gate, forget gate, output gate).\n",
        "    - These gates control the flow of information, allowing the network to retain information over longer sequences.\n",
        "\n",
        "3. **Gated Recurrent Units (GRUs)**:\n",
        "    - GRUs are a simplified version of LSTMs with fewer gates (update gate, reset gate).\n",
        "    - They perform similarly to LSTMs but are computationally more efficient.\n",
        "\n",
        "4. **Embedding Layers**:\n",
        "    - Embeddings are used to convert input sequences of tokens (like words) into dense vectors of fixed size."
      ],
      "metadata": {
        "id": "NvgFCMzBzcj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to RNNs**"
      ],
      "metadata": {
        "id": "88qsN5Ml0GrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks that are particularly effective for processing sequential data. They are widely used in tasks where the order of the data points matters, such as time series prediction, language modeling, and speech recognition.\n",
        "\n",
        "**Key Concepts of RNNs**\n",
        "\n",
        "1. **Sequential Data**:\n",
        "   - In sequential data, each data point is dependent on the previous ones. Examples include sentences (sequences of words), time series data (sequences of values), and videos (sequences of frames).\n",
        "\n",
        "2. **Recurrent Connections**:\n",
        "   - Unlike traditional feedforward neural networks, RNNs have connections that loop back on themselves. This allows information to persist, making them suitable for handling sequences of data.\n",
        "\n",
        "3. **Hidden State**:\n",
        "   - The hidden state in an RNN acts as a memory that captures information from previous time steps. It is updated at each time step based on the current input and the previous hidden state.\n",
        "   - Equation: $ h_t = f(W_{hx} x_t + W_{hh} h_{t-1} + b_h) $, where $ h_t $ is the hidden state at time $ t $, $ x_t $ is the input at time $ t $, $ W_{hx} $ and $ W_{hh} $ are weight matrices, and $ b_h $ is a bias term.\n",
        "\n",
        "4. **Output**:\n",
        "   - The output at each time step can be computed using the hidden state.\n",
        "   - Equation: $ y_t = g(W_{hy} h_t + b_y) $, where $ y_t $ is the output at time $ t $, $ W_{hy} $ is a weight matrix, and $ b_y $ is a bias term."
      ],
      "metadata": {
        "id": "7NJoCTie1eTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "iNNCmYqN2C8V"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = 10    # Input feature size\n",
        "hidden_size = 20   # Hidden state size\n",
        "num_layers = 1     # Number of RNN layers\n",
        "output_size = 5    # Output size (e.g., number of classes)\n",
        "sequence_length = 5 # Length of input sequences\n",
        "batch_size = 3     # Batch size\n",
        "\n",
        "# Create RNN layer\n",
        "rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)"
      ],
      "metadata": {
        "id": "EW0kkHD42Gne"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input sequence (batch_size x sequence_length x input_size)\n",
        "input_seq = torch.randn(batch_size, sequence_length, input_size)  # Batch size 3, sequence length 5, input size 10"
      ],
      "metadata": {
        "id": "U_pAGqjZ2Lss"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize hidden state with zeros (num_layers, batch_size, hidden_size)\n",
        "h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "\n",
        "# Forward propagate through RNN\n",
        "out, hn = rnn(input_seq, h0)\n",
        "\n",
        "# Output shapes\n",
        "print(f\"Input shape: {input_seq.shape}\")      # Expected: (3, 5, 10)\n",
        "print(f\"RNN output shape: {out.shape}\")       # Expected: (3, 5, 20)\n",
        "print(f\"Hidden state shape: {hn.shape}\")      # Expected: (1, 3, 20) -> (num_layers, batch, hidden)\n",
        "print(f\"Output type: {type(out)}\")            # Expected: <class 'torch.Tensor'>\n",
        "print(f\"Hidden state type: {type(hn)}\")       # Expected: <class 'torch.Tensor'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XacWkd02Zzf",
        "outputId": "9e102c28-bcf9-40f7-8087-59ec3ce8de0c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([3, 5, 10])\n",
            "RNN output shape: torch.Size([3, 5, 20])\n",
            "Hidden state shape: torch.Size([1, 3, 20])\n",
            "Output type: <class 'torch.Tensor'>\n",
            "Hidden state type: <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Long Short-Term Memory (LSTM) networks**"
      ],
      "metadata": {
        "id": "8--2SmpW4rUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) capable of learning long-term dependencies, particularly useful for sequential data. PyTorch, a popular deep learning library, provides robust support for implementing LSTMs."
      ],
      "metadata": {
        "id": "dNkv5gWQ4uQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM Equations**\n",
        "\n",
        "An LSTM unit consists of a cell, an input gate, a forget gate, and an output gate. These gates control the flow of information and help maintain long-term dependencies.\n",
        "\n",
        "1. **Forget Gate**: Decides what information to discard from the cell state.\n",
        "   $$\n",
        "   f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "   $$\n",
        "\n",
        "2. **Input Gate**: Decides which new information to store in the cell state.\n",
        "   $$\n",
        "   i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "   $$\n",
        "   $$\n",
        "   \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
        "   $$\n",
        "\n",
        "3. **Cell State Update**: Combines the previous cell state and the new candidate values.\n",
        "   $$\n",
        "   C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
        "   $$\n",
        "\n",
        "4. **Output Gate**: Decides what the next hidden state should be.\n",
        "   $$\n",
        "   o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "   $$\n",
        "   $$\n",
        "   h_t = o_t * \\tanh(C_t)\n",
        "   $$\n",
        "\n",
        "**Legend**\n",
        "- $ x_t $: Input at time step $ t $\n",
        "- $ h_t $: Hidden state at time step $ t $\n",
        "- $ C_t $: Cell state at time step $ t $\n",
        "- $ \\sigma $: Sigmoid function\n",
        "- $ \\tanh $: Hyperbolic tangent function\n",
        "- $ W $ and $ b $: Weight matrices and biases for respective gates\n",
        "\n",
        "**Summary**\n",
        "\n",
        "The LSTM architecture is designed to overcome the limitations of standard RNNs in capturing long-term dependencies by introducing a more complex cell state and three gates (forget, input, output) that regulate the flow of information. This makes LSTMs particularly effective for tasks involving long sequences of data."
      ],
      "metadata": {
        "id": "y8Skkmdl7XlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Ix-9A8G64ycJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "tljWtzxF413s"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.rand(100, 10, 1)  # 100 sequences, each of length 10 with 1 feature\n",
        "y_train = np.random.rand(100, 1)      # 100 target values\n",
        "\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()"
      ],
      "metadata": {
        "id": "HsGe79m45D-f"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "oJ4c1rpk5MtJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "GRr4Eh7w5P24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = model(X_train)\n",
        "    test_loss = criterion(test_output, y_train)\n",
        "    print(f'Test Loss: {test_loss.item():.4f}')"
      ],
      "metadata": {
        "id": "x5wCUbPA5ZuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gated Recurrent Unit (GRU)**"
      ],
      "metadata": {
        "id": "tAsxdyvd6dtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gated Recurrent Units (GRUs) are a variant of Recurrent Neural Networks (RNNs) designed to capture long-term dependencies more effectively than vanilla RNNs, while being simpler and computationally more efficient than Long Short-Term Memory (LSTM) networks."
      ],
      "metadata": {
        "id": "JCUYNf3m6fKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRU Equations**\n",
        "\n",
        "A GRU unit consists of a reset gate and an update gate, which control the flow of information and help maintain long-term dependencies.\n",
        "\n",
        "1. **Reset Gate**: Decides what part of the previous hidden state to forget.\n",
        "   $$\n",
        "   r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
        "   $$\n",
        "\n",
        "2. **Update Gate**: Decides how much of the previous hidden state to keep.\n",
        "   $$\n",
        "   z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
        "   $$\n",
        "\n",
        "3. **Candidate Hidden State**: Combines the reset gate with the previous hidden state and current input to create a candidate hidden state.\n",
        "   $$\n",
        "   \\tilde{h}_t = \\tanh(W_h \\cdot [r_t * h_{t-1}, x_t] + b_h)\n",
        "   $$\n",
        "\n",
        "4. **Final Hidden State**: Interpolates between the previous hidden state and the candidate hidden state using the update gate.\n",
        "   $$\n",
        "   h_t = (1 - z_t) * h_{t-1} + z_t * \\tilde{h}_t\n",
        "   $$\n",
        "\n",
        "**Legend**\n",
        "\n",
        "- $ x_t $: Input at time step $ t $\n",
        "- $ h_t $: Hidden state at time step $ t $\n",
        "- $ \\sigma $: Sigmoid function\n",
        "- $ \\tanh $: Hyperbolic tangent function\n",
        "- $ W $ and $ b $: Weight matrices and biases for respective gates\n",
        "\n",
        "**Summary**\n",
        "\n",
        "The GRU architecture simplifies the LSTM by combining the forget and input gates into a single update gate, and the cell state and hidden state into a single hidden state. This makes GRUs less complex and computationally more efficient while still being effective at capturing long-term dependencies."
      ],
      "metadata": {
        "id": "rPaVKIY38HHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "M3tKqhVP6lIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "WFfUWu0w6oNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X_train = np.random.rand(100, 10, 1)  # 100 sequences, each of length 10 with 1 feature\n",
        "y_train = np.random.rand(100, 1)      # 100 target values\n",
        "\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()"
      ],
      "metadata": {
        "id": "Sq3_CCy562pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 1\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "num_layers = 2\n",
        "\n",
        "model = GRUModel(input_size, hidden_size, output_size, num_layers)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "89hi-LQm66bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "r6papFoP67gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = model(X_train)\n",
        "    test_loss = criterion(test_output, y_train)\n",
        "    print(f'Test Loss: {test_loss.item():.4f}')"
      ],
      "metadata": {
        "id": "RDpAkxuR6_n2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}