{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "\n",
        "**1. Custom layers and models**\n",
        "\n",
        "**2. Callbacks and custom training loops**\n",
        "\n",
        "**3. Model saving and loading**\n"
      ],
      "metadata": {
        "id": "kSqZ30rHWzs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom layer and models**"
      ],
      "metadata": {
        "id": "V18zvmCXW9WG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating custom layers and models in PyTorch allows you to tailor the architecture to specific needs and use cases. Here's a guide on how to create custom layers and models in PyTorch.\n",
        "\n",
        "**1. Custom Layers**\n",
        "\n",
        "To create a custom layer, you need to subclass torch.nn.Module and define the __init__ and forward methods.\n",
        "Example: Custom Linear Layer\n",
        "\n",
        "Let's create a simple custom linear layer with optional bias:"
      ],
      "metadata": {
        "id": "cNoNqCutXARu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HzaGm2aVKty",
        "outputId": "d20e94aa-5624-41ea-cb48-2fe2c7bbf7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0410,  0.9374],\n",
            "        [ 0.4434,  0.2989],\n",
            "        [-0.7112, -0.3032],\n",
            "        [ 0.9502,  0.4853],\n",
            "        [ 1.0396,  0.0723],\n",
            "        [ 1.3830, -0.8336],\n",
            "        [ 0.4555,  1.2553],\n",
            "        [ 1.1225,  0.4792],\n",
            "        [ 0.9487,  0.6052],\n",
            "        [ 0.0253,  0.6532]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class CustomLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(CustomLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.nn.functional.linear(input, self.weight, self.bias)\n",
        "\n",
        "# Example usage\n",
        "input = torch.randn(10, 5)\n",
        "layer = CustomLinear(5, 2)\n",
        "output = layer(input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Custom Models**\n",
        "\n",
        "To create a custom model, you also subclass torch.nn.Module and define the __init__ and forward methods. You can use standard and custom layers within your model.\n",
        "Example: Custom Neural Network **Model**"
      ],
      "metadata": {
        "id": "zaPVl8PcXxqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "model = CustomModel()\n",
        "input = torch.randn(1, 1, 28, 28)\n",
        "output = model(input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_CgvspzX33I",
        "outputId": "da4a95a5-a5f6-4117-d83a-843a4b94d2a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1139, -0.0751, -0.2225, -0.0489, -0.1548, -0.1177,  0.2606, -0.0391,\n",
            "          0.1103, -0.0460]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Training Custom Models**\n",
        "\n",
        "To train your custom model, you follow the standard PyTorch training loop:"
      ],
      "metadata": {
        "id": "S2FuPLkQZwTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 10\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "3ht8W9YsZ2Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example covers the basics of creating custom layers and models in PyTorch. You can expand on these concepts by adding more complexity, such as additional layers, custom operations, and advanced training techniques."
      ],
      "metadata": {
        "id": "c3FIajx8abaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Callbacks and custom training loops**"
      ],
      "metadata": {
        "id": "FjmptH8raucn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callbacks and custom training loops in PyTorch offer flexibility for implementing complex training logic, such as learning rate scheduling, early stopping, and custom logging. PyTorch does not have built-in support for callbacks like some other deep learning frameworks (e.g., Keras), but you can implement this functionality manually within your training loop."
      ],
      "metadata": {
        "id": "I61v7ZogcZvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Custom Training Loop**\n",
        "\n",
        "A custom training loop gives you full control over the training process. Below is an example of a custom training loop in PyTorch:\n",
        "Example: Custom Training Loop"
      ],
      "metadata": {
        "id": "LBJYBZotcas4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Example dataset and dataloader\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = torch.randn(1000, 1, 28, 28)\n",
        "        self.targets = torch.randint(0, 10, (1000,))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.targets[index]\n",
        "\n",
        "train_dataset = DummyDataset()\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Custom model\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = CustomModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Custom training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "5XELEAWqcZEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Implementing Callbacks**\n",
        "\n",
        "Callbacks can be implemented by creating custom classes that handle specific actions during the training loop. Here are some examples of common callbacks:\n",
        "\n",
        "**Example: Early Stopping Callback**"
      ],
      "metadata": {
        "id": "inuCxf1RcuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# Usage in training loop\n",
        "early_stopping = EarlyStopping(patience=3, delta=0.01)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    val_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {val_loss}\")\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break"
      ],
      "metadata": {
        "id": "XyPWKmWgc8pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Learning Rate Scheduler Callback**"
      ],
      "metadata": {
        "id": "X66d9SInd9Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LRScheduler:\n",
        "    def __init__(self, optimizer, patience=5, factor=0.5, min_lr=1e-6):\n",
        "        self.optimizer = optimizer\n",
        "        self.patience = patience\n",
        "        self.factor = factor\n",
        "        self.min_lr = min_lr\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.reduce_lr()\n",
        "                self.counter = 0\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "    def reduce_lr(self):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            new_lr = max(param_group['lr'] * self.factor, self.min_lr)\n",
        "            print(f\"Reducing learning rate to {new_lr}\")\n",
        "            param_group['lr'] = new_lr\n",
        "\n",
        "# Usage in training loop\n",
        "lr_scheduler = LRScheduler(optimizer, patience=3, factor=0.5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    val_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {val_loss}\")\n",
        "\n",
        "    lr_scheduler.step(val_loss)"
      ],
      "metadata": {
        "id": "6wTeHtkYd-Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Combining Callbacks**\n",
        "\n",
        "You can combine multiple callbacks in a single training loop for more complex training logic:"
      ],
      "metadata": {
        "id": "rgD8h78BeNtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(patience=3, delta=0.01)\n",
        "lr_scheduler = LRScheduler(optimizer, patience=3, factor=0.5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    val_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {val_loss}\")\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    lr_scheduler.step(val_loss)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "9U49Aoi5eQLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model saving and loading**"
      ],
      "metadata": {
        "id": "huDEUB07eVJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving and loading models in PyTorch is straightforward and essential for tasks such as model evaluation, resuming training, and deploying models. Hereâ€™s a detailed guide on how to save and load models in PyTorch."
      ],
      "metadata": {
        "id": "_nO-VAyWfRTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>1. Saving and Loading Model State Dict</h2>**\n",
        "\n",
        "The recommended approach for saving and loading models in PyTorch is using the state_dict. The state_dict is a Python dictionary object that maps each layer to its parameter tensor.\n",
        "\n",
        "**Saving Model State Dict**"
      ],
      "metadata": {
        "id": "UoC5FRxYfXYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example model\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CustomModel()\n",
        "\n",
        "# Save the model state dict\n",
        "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
        "print(\"Model state dict saved!\")"
      ],
      "metadata": {
        "id": "kBo6bi5Mf2cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Model State Dict**"
      ],
      "metadata": {
        "id": "OIm6IN2Sf-km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new instance of the model\n",
        "model = CustomModel()\n",
        "\n",
        "# Load the saved state dict\n",
        "model.load_state_dict(torch.load('model_state_dict.pth'))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "print(\"Model state dict loaded!\")"
      ],
      "metadata": {
        "id": "4c--X6cLgBaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>2. Saving and Loading Entire Model</h2>**\n",
        "\n",
        "While saving the entire model (including architecture and state) is possible, it is generally not recommended because it can cause issues during loading due to changes in the code structure. However, it might be useful for quick experiments or small projects.\n",
        "\n",
        "**Saving Entire Model**"
      ],
      "metadata": {
        "id": "EZ7h_fHCgO6j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Ooga8QZgjUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model, 'entire_model.pth')\n",
        "print(\"Entire model saved!\")"
      ],
      "metadata": {
        "id": "nzRPAOUPgW0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Entire Model**"
      ],
      "metadata": {
        "id": "jFb9cA9ygZsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire model\n",
        "model = torch.load('entire_model.pth')\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "print(\"Entire model loaded!\")"
      ],
      "metadata": {
        "id": "acaiSXUlgakS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>3. Saving and Loading Checkpoints</h2>**\n",
        "\n",
        "Saving and loading checkpoints is useful for resuming training from a specific point. A checkpoint usually includes the model state dict, optimizer state dict, and other training information.\n",
        "\n",
        "**Saving Checkpoint**"
      ],
      "metadata": {
        "id": "PbkyvHzsgerG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "epoch = 5\n",
        "loss = 0.5\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, 'checkpoint.pth')\n",
        "print(\"Checkpoint saved!\")"
      ],
      "metadata": {
        "id": "aHGOcHP8glKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Checkpoint**"
      ],
      "metadata": {
        "id": "t_LdS53UgsxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('checkpoint.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.train()  # Set the model to training mode if resuming training\n",
        "print(f\"Checkpoint loaded! Resuming from epoch {epoch} with loss {loss}.\")"
      ],
      "metadata": {
        "id": "x-4UY76Pgvgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>4. Example Usage: Training and Saving a Model</h2>**\n",
        "\n",
        "Here's a complete example of training a model, saving the state dict, and then loading it:"
      ],
      "metadata": {
        "id": "wNWtMytdg3GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Dummy dataset\n",
        "x = torch.randn(1000, 1, 28, 28)\n",
        "y = torch.randint(0, 10, (1000,))\n",
        "dataset = TensorDataset(x, y)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = CustomModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Save the model state dict\n",
        "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
        "print(\"Model state dict saved!\")\n",
        "\n",
        "# Load the model state dict\n",
        "model.load_state_dict(torch.load('model_state_dict.pth'))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "print(\"Model state dict loaded!\")"
      ],
      "metadata": {
        "id": "Pb68bNuag8He"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}