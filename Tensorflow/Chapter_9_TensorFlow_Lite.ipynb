{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Introduction to TensorFlow Lite**\n",
        "\n",
        "**2. Model conversion for mobile and edge devices**\n",
        "\n",
        "**3. Optimizing models for deployment on resource-constrained devices**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4EUWQZecQsJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Introduction to TensorFlow Lite</h2>**\n",
        "\n",
        "TensorFlow Lite (TFLite) is a lightweight, cross-platform solution for deploying machine learning models on mobile, embedded, and IoT devices. It optimizes TensorFlow models for efficiency and performance, allowing for inference on devices with limited resources."
      ],
      "metadata": {
        "id": "vFQH7kpQaQ2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Steps to Convert and Deploy a Model with TensorFlow Lite</h2>**"
      ],
      "metadata": {
        "id": "tRPML_1NaVlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Train or Load a TensorFlow Model**\n",
        "\n",
        "First, you need to have a trained TensorFlow model. You can either train a new model or load a pre-trained model."
      ],
      "metadata": {
        "id": "6x2dsEt-alA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Generate some dummy data\n",
        "x_train = np.random.random((100, 4))\n",
        "y_train = np.random.random((100, 1))\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "id": "ibyMZKRaangf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Convert the Model to TensorFlow Lite Format**\n",
        "\n",
        "After training or loading your model, convert it to the TensorFlow Lite format."
      ],
      "metadata": {
        "id": "VRmDJPy_av5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the model to the TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to a file\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "5ZAVOUG3ay3g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Load the TFLite Model**\n",
        "\n",
        "To use the TFLite model, you need to load it into an interpreter."
      ],
      "metadata": {
        "id": "O8DJl4vha_ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
        "interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "h1_TbVEWbBap"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Prepare Input Data**\n",
        "\n",
        "Input data needs to be prepared and formatted correctly for the model."
      ],
      "metadata": {
        "id": "W54DDoDobDm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare input data (using the same dummy data from training)\n",
        "input_data = np.array([[0.1, 0.2, 0.3, 0.4]], dtype=np.float32)"
      ],
      "metadata": {
        "id": "53uBLckBbJ63"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Run Inference with TFLite Model**\n",
        "\n",
        "Run the inference on the input data using the TFLite interpreter."
      ],
      "metadata": {
        "id": "QwP-2-rDbPEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Invoke the interpreter\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the prediction\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5fTGt3ebSgJ",
        "outputId": "851d81ae-bfc4-43a0-895d-973a9b21a81c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.23970504]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Model conversion for mobile and edges devices</h2>**\n",
        "\n",
        "Model conversion for mobile and edge devices involves transforming a trained TensorFlow model into a format that can be efficiently executed on devices with limited computational and memory resources. TensorFlow Lite (TFLite) is designed for this purpose."
      ],
      "metadata": {
        "id": "VBayXBEkdlEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Train a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "x_train = np.random.random((100, 4))\n",
        "y_train = np.random.random((100, 1))\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Step 2: Convert the model to TensorFlow Lite format with quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Quantization\n",
        "tflite_model = converter.convert()\n",
        "with open('model_quant.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Step 3: Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path='model_quant.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Step 4: Prepare input data\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_data = np.array([[0.1, 0.2, 0.3, 0.4]], dtype=np.float32)\n",
        "\n",
        "# Step 5: Run inference\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "metadata": {
        "id": "VZOxhMtgdva9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Optimizing models for deployment on resource-constrained devices</h2>**"
      ],
      "metadata": {
        "id": "34az4yfTeyE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing models for deployment on resource-constrained devices involves transforming and enhancing machine learning models to run efficiently on devices with limited computational, memory, and power resources. Techniques include **quantization**, **pruning**, and **model architecture optimization**."
      ],
      "metadata": {
        "id": "meyqwFqJe2JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**1. Quantization**\n",
        "\n",
        "Quantization reduces the precision of the numbers used to represent a modelâ€™s parameters, which can significantly reduce the model size and improve inference speed.\n",
        "\n",
        "**Example: Post-Training Quantization**\n",
        "\n",
        "Post-training quantization converts a pre-trained model to use lower precision (e.g., 8-bit integers instead of 32-bit floats).\n",
        "\n",
        "**Steps**:\n",
        "\n",
        "1. Train or load a TensorFlow model.\n",
        "2. Convert the model with quantization.\n",
        "\n"
      ],
      "metadata": {
        "id": "ux-I503sfESY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Train a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "x_train = np.random.random((100, 4))\n",
        "y_train = np.random.random((100, 1))\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Step 2: Convert the model to TFLite format with post-training quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open('model_quant.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path='model_quant.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Prepare input data\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_data = np.array([[0.1, 0.2, 0.3, 0.4]], dtype=np.float32)\n",
        "\n",
        "# Run inference\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "id": "bD9PYeYHfZri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**2. Pruning**\n",
        "\n",
        "Pruning removes weights that contribute less to the overall output, thus reducing the model size and computational cost.\n",
        "\n",
        "**Example: Model Pruning**\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "- Apply pruning during training.\n",
        "- Fine-tune the pruned model.\n",
        "- Convert the pruned model to TFLite."
      ],
      "metadata": {
        "id": "wqEmwKPsfp2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "QPA1TA69gMtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Step 1: Train a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "x_train = np.random.random((100, 4))\n",
        "y_train = np.random.random((100, 1))\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Step 2: Apply pruning\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.0, final_sparsity=0.5,\n",
        "        begin_step=0, end_step=len(x_train) // 32 * 5\n",
        "    )\n",
        "}\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "model_for_pruning.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Step 3: Fine-tune the pruned model\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "    tfmot.sparsity.keras.PruningSummaries(log_dir='pruning_logs')\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(x_train, y_train, epochs=2, callbacks=callbacks)\n",
        "\n",
        "# Step 4: Strip pruning wrappers and convert to TFLite\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the pruned and converted model\n",
        "with open('model_pruned.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path='model_pruned.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Prepare input data\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_data = np.array([[0.1, 0.2, 0.3, 0.4]], dtype=np.float32)\n",
        "\n",
        "# Run inference\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg3hfuRZf7zv",
        "outputId": "f61a123f-3797-4076-9bee-a5e95c14d1c6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 0.2419\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.2235\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.2073\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1925\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.1797\n",
            "Epoch 1/2\n",
            "4/4 [==============================] - 2s 6ms/step - loss: 0.1693\n",
            "Epoch 2/2\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1433"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 0.0053s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 0s 6ms/step - loss: 0.1588\n",
            "[[0.19080383]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Model Architecture Optimization**\n",
        "\n",
        "Choosing or designing model architectures that are inherently efficient and lightweight is crucial for deployment on resource-constrained devices. This includes using models like MobileNet, SqueezeNet, and EfficientNet.\n",
        "\n",
        "**Example: Using a Pre-Trained MobileNet Model**\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "- Load a pre-trained MobileNet model.\n",
        "- Convert the model to TFLite format.\n",
        "- Load and run inference."
      ],
      "metadata": {
        "id": "zwwdBz5wgguG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Load a pre-trained MobileNet model\n",
        "model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=True, weights='imagenet')\n",
        "\n",
        "# Step 2: Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model\n",
        "with open('mobilenet_v2.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Step 3: Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path='mobilenet_v2.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Prepare input data\n",
        "input_data = np.random.random((1, 224, 224, 3)).astype(np.float32)\n",
        "\n",
        "# Run inference\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)\n"
      ],
      "metadata": {
        "id": "AuO91WmJgtfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzJrQayrg-HJ",
        "outputId": "2d894844-d2c2-4c62-b249-4c990469e1aa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Edge TPU Compatibility**\n",
        "\n",
        "For devices with Edge TPUs, such as Google Coral, models need to be compiled specifically for the TPU hardware.\n",
        "\n",
        "**Example: Compiling for Edge TPU**\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "- Convert the model to TFLite format.\n",
        "- Compile the TFLite model for Edge TPU using the Edge TPU compiler."
      ],
      "metadata": {
        "id": "HqWJR097hHRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the quantized TFLite model as a file (assume `tflite_model` from previous examples)\n",
        "with open('model_quant.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Compile the TFLite model for Edge TPU\n",
        "!edgetpu_compiler model_quant.tflite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndYjyO64hUJY",
        "outputId": "86427b51-4657-4a95-d282-220a8e60b659"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: edgetpu_compiler: command not found\n"
          ]
        }
      ]
    }
  ]
}