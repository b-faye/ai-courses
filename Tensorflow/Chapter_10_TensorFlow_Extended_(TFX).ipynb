{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Introduction to TensorFlow Extended**\n",
        "\n",
        "**2. Building end-to-end machine learning pipelines**\n",
        "    \n",
        "**3. Model versioning and deployment in production**\n",
        "\n"
      ],
      "metadata": {
        "id": "4EUWQZecQsJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Introduction to TensorFlow Extended</h2>**"
      ],
      "metadata": {
        "id": "-wWB1R2jlDvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow Extended (TFX) is an end-to-end platform for deploying production machine learning (ML) pipelines. It provides a configuration-driven framework to orchestrate and automate machine learning workflows.\n",
        "Components:\n",
        "\n",
        "- TensorFlow Data Validation (TFDV): For data analysis and validation.\n",
        "- TensorFlow Transform (TFT): For feature engineering.\n",
        "- TensorFlow Model Analysis (TFMA): For model evaluation.\n",
        "- TensorFlow Serving: For serving models in production.\n",
        "- ML Metadata (MLMD): For tracking metadata of machine learning workflows."
      ],
      "metadata": {
        "id": "adm-ViHklJuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Building end-to-end machine learning pipelines</h2>**"
      ],
      "metadata": {
        "id": "3lI-XXbz8GL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building an end-to-end machine learning pipeline with TensorFlow Extended (TFX) involves several components to manage the lifecycle of a machine learning model, from data ingestion to model serving. Let's walk through a real-world example of using TFX to build a pipeline for a supervised learning taskâ€”predicting house prices.\n",
        "\n",
        "**Scenario**\n",
        "You have a dataset of house features (e.g., number of bedrooms, square footage) and prices. You want to build a pipeline that:\n",
        "1. Ingests data.\n",
        "2. Preprocesses it.\n",
        "3. Trains a model.\n",
        "4. Evaluates the model.\n",
        "5. Serves the model.\n",
        "\n",
        "**Components of a TFX Pipeline**\n",
        "\n",
        "1. **ExampleGen**: Ingests raw data into the pipeline.\n",
        "2. **ExampleValidator**: Validates the schema of the data.\n",
        "3. **SchemaGen**: Generates a schema for data validation.\n",
        "4. **ExampleTransform**: Transforms raw data into features.\n",
        "5. **Trainer**: Trains the model.\n",
        "6. **Evaluator**: Evaluates the model's performance.\n",
        "7. **InfraValidator**: Validates the model for serving.\n",
        "8. **Pusher**: Pushes the model to a serving location.\n",
        "\n"
      ],
      "metadata": {
        "id": "QHGVqvj59kk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Set Up the Environment**\n",
        "\n",
        "Ensure you have the required packages installed:"
      ],
      "metadata": {
        "id": "e2LAknF090jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tfx apache-beam tensorflow"
      ],
      "metadata": {
        "id": "iVj2yWWr9uMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Define the Pipeline**\n",
        "\n",
        "Create a Python script (pipeline.py) to define your TFX pipeline:"
      ],
      "metadata": {
        "id": "Q7zgxThFDPNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tfx\n",
        "from tfx.components import CsvExampleGen, ExampleValidator, SchemaGen, ExampleTransform, Trainer, Evaluator, InfraValidator, Pusher\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.utils.dsl_utils import tfrecordio\n",
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "# Define constants\n",
        "PIPELINE_NAME = 'house_price_pipeline'\n",
        "PIPELINE_ROOT = '/path/to/pipeline_root'\n",
        "DATA_ROOT = '/path/to/data'\n",
        "MODULE_FILE = '/path/to/model_module.py'\n",
        "SERVING_MODEL_DIR = '/path/to/serving_model_dir'\n",
        "\n",
        "def create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str, module_file: str, serving_model_dir: str) -> tfx.dsl.Pipeline:\n",
        "    # Ingest data\n",
        "    example_gen = CsvExampleGen(input_base=data_root)\n",
        "\n",
        "    # Validate data\n",
        "    example_validator = ExampleValidator(examples=example_gen.outputs['examples'])\n",
        "    schema_gen = SchemaGen(stats=example_validator.outputs['stats'], infer_schema=True)\n",
        "\n",
        "    # Transform data\n",
        "    example_transform = ExampleTransform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        module_file=module_file\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trainer = Trainer(\n",
        "        module_file=module_file,\n",
        "        transformed_examples=example_transform.outputs['transformed_examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        transform_graph=example_transform.outputs['transform_graph'],\n",
        "        train_args=trainer_pb2.TrainArgs(num_steps=1000),\n",
        "        eval_args=trainer_pb2.EvalArgs(num_steps=500)\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        model_exports=trainer.outputs['model'],\n",
        "        eval_config=None  # Define your evaluation config if needed\n",
        "    )\n",
        "\n",
        "    # Validate the model for serving\n",
        "    infra_validator = InfraValidator(model=trainer.outputs['model'])\n",
        "\n",
        "    # Push model to serving\n",
        "    pusher = Pusher(\n",
        "        model=trainer.outputs['model'],\n",
        "        model_blessing=evaluator.outputs['blessing'],\n",
        "        push_destination=tfx.proto.PushDestination(\n",
        "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "                base_directory=serving_model_dir\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return tfx.dsl.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=[\n",
        "            example_gen,\n",
        "            example_validator,\n",
        "            schema_gen,\n",
        "            example_transform,\n",
        "            trainer,\n",
        "            evaluator,\n",
        "            infra_validator,\n",
        "            pusher\n",
        "        ],\n",
        "        # Optionally, specify the Beam pipeline runner and other parameters\n",
        "    )\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == '__main__':\n",
        "    pipeline = create_pipeline(\n",
        "        PIPELINE_NAME,\n",
        "        PIPELINE_ROOT,\n",
        "        DATA_ROOT,\n",
        "        MODULE_FILE,\n",
        "        SERVING_MODEL_DIR\n",
        "    )\n",
        "    BeamDagRunner().run(pipeline)"
      ],
      "metadata": {
        "id": "7LuS7DL1DH6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Implement the Model Module**\n",
        "\n",
        "Create a file (model_module.py) defining your model and preprocessing:"
      ],
      "metadata": {
        "id": "d2AiZfi7DVWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "from tfx.utils import proto_utils\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    # Define your preprocessing steps here\n",
        "    return inputs\n",
        "\n",
        "def _model_fn(features: tf.Tensor, labels: tf.Tensor, mode: tf.estimator.ModeKeys):\n",
        "    # Define your model architecture here\n",
        "    feature_columns = [tf.feature_column.numeric_column(key='some_feature')]\n",
        "    estimator = tf.estimator.DNNRegressor(feature_columns=feature_columns)\n",
        "    return estimator\n",
        "\n",
        "def run_fn(fn_args: FnArgs):\n",
        "    # Model training logic\n",
        "    estimator = _model_fn(fn_args)\n",
        "    estimator.train(input_fn=fn_args.train_files, steps=1000)\n",
        "    # Save model and other artifacts\n"
      ],
      "metadata": {
        "id": "zKKqEhPEDZ_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Execute the Pipeline**\n",
        "\n",
        "Run the pipeline script:"
      ],
      "metadata": {
        "id": "Ce_OTmKZDehO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python pipeline.py\n"
      ],
      "metadata": {
        "id": "x_D2aPUdDiKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Model versioning and deployment in production</h2>**"
      ],
      "metadata": {
        "id": "iqzEwbZJD2Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Model Versioning**\n",
        "\n",
        "**Model versioning** refers to tracking and managing different versions of a machine learning model. Proper versioning ensures that you can roll back to previous versions, track changes, and maintain consistency across environments.\n",
        "\n",
        "**Strategies for Model Versioning**\n",
        "\n",
        "1. **Version Naming Convention**\n",
        "   - **Semantic Versioning**: Use a versioning scheme like `1.0.0`, `1.1.0`, `2.0.0` to indicate major, minor, and patch changes.\n",
        "   - **Date-based Versioning**: Include date in the versioning, such as `2024-07-22`.\n",
        "\n",
        "2. **Metadata Management**\n",
        "   - **Model Metadata**: Track metadata such as training dataset versions, hyperparameters, and evaluation metrics.\n",
        "   - **Model Artifacts**: Store the model weights, architecture, and associated files.\n",
        "\n",
        "3. **Version Control Systems**\n",
        "   - **Git**: Use Git to version control model code and configuration.\n",
        "   - **DVC (Data Version Control)**: For versioning large datasets and model binaries.\n",
        "\n",
        "4. **Model Registry**\n",
        "   - **TFX ModelRegistry**: TFX includes a model registry where you can keep track of different model versions.\n",
        "   - **MLflow**: A popular open-source tool that offers model tracking, versioning, and deployment features.\n",
        "   - **SageMaker Model Registry**: If using AWS SageMaker, it provides model versioning and management.\n",
        "\n"
      ],
      "metadata": {
        "id": "X1XhwN4oD5Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example with MLflow:**"
      ],
      "metadata": {
        "id": "VuVVAvWbEdR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "\n",
        "# Log model and parameters\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.log_param(\"param1\", value1)\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.sklearn.log_model(model, \"model\")\n",
        "\n",
        "    # Retrieve the model version\n",
        "    model_version = mlflow.register_model(\"runs:/<RUN_ID>/model\", \"model_name\")"
      ],
      "metadata": {
        "id": "60-JqiScEVzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Model Deployment**\n"
      ],
      "metadata": {
        "id": "eT02JCLFEkzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment Strategies**\n",
        "\n",
        "1. **Blue-Green Deployment**\n",
        "   - Deploy the new model version alongside the old one.\n",
        "   - Switch traffic from the old version to the new one once the new model is verified.\n",
        "\n",
        "2. **Canary Deployment**\n",
        "   - Deploy the new model to a small subset of users.\n",
        "   - Gradually roll out the model to all users if the new version performs well.\n",
        "\n",
        "3. **Rolling Update**\n",
        "   - Gradually replace the old model with the new one across all instances.\n",
        "\n",
        "4. **Shadow Deployment**\n",
        "   - Deploy the new model and run it in parallel with the old model.\n",
        "   - Compare performance metrics without affecting the production traffic.\n",
        "\n",
        "5. **A/B Testing**\n",
        "   - Serve multiple models to different user segments and compare their performance.\n",
        "\n",
        "**Tools for Deployment**\n",
        "\n",
        "1. **TFX Serving**\n",
        "   - TFX pipelines include model serving components, and you can deploy them on TensorFlow Serving.\n",
        "   - **TensorFlow Serving**: A flexible serving system for machine learning models, designed for production environments.\n",
        "\n",
        "2. **KubeFlow**\n",
        "   - Provides end-to-end solutions for model deployment and management in Kubernetes.\n",
        "\n",
        "3. **MLflow**\n",
        "   - Deploy models as REST APIs or batch jobs. Supports multiple frameworks and deployment targets.\n",
        "\n",
        "4. **SageMaker**\n",
        "   - Amazon SageMaker offers a managed service for deploying models with features for automatic scaling and monitoring.\n",
        "\n",
        "**Example with TensorFlow Serving:**\n",
        "\n",
        "```bash\n",
        "# Run TensorFlow Serving with Docker\n",
        "docker run -p 8501:8501 --name=tf_model_serving \\\n",
        "    -v \"/path/to/model:/models/model_name\" \\\n",
        "    -e MODEL_NAME=model_name \\\n",
        "    tensorflow/serving:latest\n",
        "```\n",
        "\n",
        "**Example with MLflow:**\n"
      ],
      "metadata": {
        "id": "Zq0iJGZAERbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow.pyfunc\n",
        "\n",
        "# Load model\n",
        "model = mlflow.pyfunc.load_model(\"models:/model_name/1\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(data)\n"
      ],
      "metadata": {
        "id": "Xq_DqJ42E7n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Monitoring and Maintenance**\n",
        "\n",
        "- **Monitor Model Performance**: Track metrics like latency, throughput, and prediction accuracy.\n",
        "- **A/B Testing Results**: Analyze performance differences between model versions.\n",
        "- **Automated Retraining**: Implement pipelines that trigger retraining based on data drift or performance degradation.\n"
      ],
      "metadata": {
        "id": "Y0w4vIJvFArg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "**Model Versioning**:\n",
        "- Use semantic or date-based versioning.\n",
        "- Track metadata and manage artifacts.\n",
        "- Utilize version control systems and model registries.\n",
        "\n",
        "**Model Deployment**:\n",
        "- Employ strategies like Blue-Green, Canary, Rolling Update, Shadow Deployment, and A/B Testing.\n",
        "- Use tools like TensorFlow Serving, KubeFlow, MLflow, or SageMaker for deployment.\n",
        "\n",
        "**Monitoring and Maintenance**:\n",
        "- Continuously monitor and evaluate model performance.\n",
        "- Use automated pipelines for retraining and updating models.\n"
      ],
      "metadata": {
        "id": "rXTTxLD4FMda"
      }
    }
  ]
}