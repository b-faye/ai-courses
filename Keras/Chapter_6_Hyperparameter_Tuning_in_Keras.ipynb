{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IQpgkg0Jv1UX",
        "CCdyGtlqwELF",
        "HpB1qUoawVjY",
        "HePolVHCwdFz",
        "_2fUM9Wlwo9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Importance of hyperparameters**\n",
        "\n",
        "**2. Techniques for hyperparameter tuning**\n",
        "\n",
        "**3. Grid search and random search with Keras**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiBklHRuTbj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importance of hyperparameters**"
      ],
      "metadata": {
        "id": "RYhKhckbpNNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters are crucial in defining the performance and behavior of machine learning models, including those built with Keras. Unlike model parameters, which are learned from the data during training, hyperparameters are set before training begins and play a significant role in shaping the model's learning process and performance.\n",
        "\n",
        "**<h2>Key Hyperparameters in Keras</h2>**\n",
        "\n",
        "1. **Learning Rate**:\n",
        "   - **Description**: Controls the size of the steps the optimizer takes during gradient descent.\n",
        "   - **Importance**: A learning rate that's too high can cause the model to converge too quickly to a suboptimal solution or even diverge. A learning rate that's too low can result in slow convergence.\n",
        "\n",
        "2. **Number of Epochs**:\n",
        "   - **Description**: The number of times the entire training dataset passes through the model.\n",
        "   - **Importance**: Too few epochs may lead to underfitting, while too many epochs can cause overfitting. Proper tuning is required to balance training time and model performance.\n",
        "\n",
        "3. **Batch Size**:\n",
        "   - **Description**: The number of samples processed before the model is updated.\n",
        "   - **Importance**: A small batch size provides a noisy estimate of the gradient, which can be beneficial for escaping local minima but may result in slower convergence. A large batch size provides a more accurate estimate of the gradient but may lead to slower convergence due to memory constraints.\n",
        "\n",
        "4. **Number of Layers and Units per Layer**:\n",
        "   - **Description**: The architecture of the neural network, including the number and type of layers and the number of units in each layer.\n",
        "   - **Importance**: Too few layers or units can lead to underfitting, while too many can lead to overfitting and increased computational cost.\n",
        "\n",
        "5. **Activation Functions**:\n",
        "   - **Description**: Functions applied to the output of each layer, such as ReLU, sigmoid, or tanh.\n",
        "   - **Importance**: The choice of activation function affects the learning dynamics and the ability of the model to capture non-linear relationships.\n",
        "\n",
        "6. **Optimizer**:\n",
        "   - **Description**: Algorithm used to update the modelâ€™s weights, such as Adam, SGD, or RMSprop.\n",
        "   - **Importance**: Different optimizers have different strategies for adjusting the learning rate and handling gradients. The choice of optimizer can significantly affect the convergence speed and final performance.\n",
        "\n",
        "7. **Dropout Rate**:\n",
        "   - **Description**: Fraction of input units to drop during training to prevent overfitting.\n",
        "   - **Importance**: A higher dropout rate can prevent overfitting but may also result in underfitting if too high.\n",
        "\n",
        "8. **Regularization Parameters**:\n",
        "   - **Description**: Techniques like L1/L2 regularization that add a penalty to the loss function to prevent overfitting.\n",
        "   - **Importance**: Regularization helps in reducing model complexity and improving generalization.\n",
        "\n",
        "9. **Learning Rate Schedulers**:\n",
        "   - **Description**: Methods to adjust the learning rate during training, such as step decay or exponential decay.\n",
        "   - **Importance**: Dynamic adjustment of learning rates can help in better convergence and faster training.\n",
        "\n",
        "**<h2>Why Hyperparameters Matter</h2>**\n",
        "\n",
        "1. **Model Performance**: Properly tuned hyperparameters can drastically improve model accuracy, reduce loss, and increase robustness. Poorly chosen hyperparameters can lead to underfitting or overfitting.\n",
        "\n",
        "2. **Training Efficiency**: Effective hyperparameter settings can lead to faster convergence, reducing the computational resources and time required to train the model.\n",
        "\n",
        "3. **Generalization**: Well-chosen hyperparameters help the model generalize better to unseen data, improving its performance on test datasets.\n",
        "\n",
        "4. **Avoiding Overfitting**: Hyperparameters such as dropout rate and regularization techniques help in preventing overfitting, where the model performs well on training data but poorly on unseen data.\n",
        "\n",
        "**<h2>Strategies for Hyperparameter Tuning</h2>**\n",
        "\n",
        "1. **Grid Search**: Exhaustively searches over a specified hyperparameter grid. It is computationally expensive but straightforward.\n",
        "\n",
        "2. **Random Search**: Samples hyperparameters randomly from a specified range. It can be more efficient than grid search and often finds good hyperparameters more quickly.\n",
        "\n",
        "3. **Bayesian Optimization**: Uses probabilistic models to find the best hyperparameters based on past results. It is more sophisticated and can be more efficient than grid or random search.\n",
        "\n",
        "4. **Hyperparameter Tuning Libraries**: Libraries like Keras Tuner, Optuna, and Ray Tune offer advanced tools for hyperparameter optimization.\n",
        "\n",
        "### Example: Tuning Hyperparameters with Keras Tuner\n",
        "\n",
        "Here's an example of using Keras Tuner to optimize hyperparameters:\n",
        "\n",
        "```python\n",
        "import kerastuner as kt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.applications import VGG16\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=512, step=32), activation='relu'))\n",
        "    model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=kt.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling='LOG')),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Retrieve the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "```\n",
        "\n",
        "**<h2>Summary</h2>**\n",
        "\n",
        "Hyperparameters are essential in determining how well a model performs, how efficiently it trains, and how well it generalizes to new data. Proper tuning of hyperparameters can significantly improve model performance and training efficiency. Techniques like grid search, random search, and Bayesian optimization can help in finding the optimal set of hyperparameters for your model."
      ],
      "metadata": {
        "id": "XmdZxBbYpsGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Techniques for hyperparameter tuning**"
      ],
      "metadata": {
        "id": "rWhMEF2Np5EK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is a crucial step in optimizing the performance of machine learning models. In Keras, several techniques can be used to find the best hyperparameters for your model. Here are the most common methods:\n",
        "\n",
        "**<h2>1. Grid Search</h2>**\n",
        "\n",
        "Grid Search involves specifying a grid of hyperparameter values and exhaustively evaluating all possible combinations. While it can be computationally expensive, it is straightforward and ensures that all specified values are considered.\n",
        "\n",
        "**Steps:**\n",
        "1. Define a parameter grid.\n",
        "2. Train and evaluate models for each combination in the grid.\n",
        "3. Select the combination that performs best based on a validation metric.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=64, activation='relu', input_shape=(input_shape,)))\n",
        "    model.add(Dense(units=10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "```\n",
        "\n",
        "**<h2>2. Random Search</h2>**\n",
        "\n",
        "Random Search samples a fixed number of hyperparameter combinations from a predefined range. It is generally more efficient than Grid Search, especially with large parameter spaces.\n",
        "\n",
        "**Steps:**\n",
        "1. Define the parameter space with ranges or distributions.\n",
        "2. Randomly sample combinations and evaluate them.\n",
        "3. Select the best-performing combination.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from scipy.stats import uniform\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=64, activation='relu', input_shape=(input_shape,)))\n",
        "    model.add(Dense(units=10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "param_dist = {\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20],\n",
        "    'dropout_rate': uniform(0, 0.5)  # Example of a continuous distribution\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3)\n",
        "random_search_result = random_search.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")\n",
        "```\n",
        "\n",
        "**<h2>3. Bayesian Optimization</h2>**\n",
        "\n",
        "Bayesian Optimization uses probabilistic models to explore the hyperparameter space more efficiently. It builds a model of the function to optimize and uses it to choose the most promising hyperparameters to evaluate next.\n",
        "\n",
        "**Libraries**:\n",
        "- **Keras Tuner**: A library integrated with Keras that supports Bayesian optimization.\n",
        "- **Optuna**: An advanced hyperparameter optimization framework.\n",
        "\n",
        "**Example with Keras Tuner:**\n",
        "```python\n",
        "import kerastuner as kt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu', input_shape=(input_shape,)))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(\n",
        "        optimizer=kt.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling='LOG')),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "```\n",
        "\n",
        "**<h2>4. Hyperband</h2>**\n",
        "\n",
        "Hyperband is an adaptive resource allocation method that combines random search with early stopping. It allocates resources to the most promising configurations and terminates less promising ones early.\n",
        "\n",
        "**Example with Keras Tuner:**\n",
        "```python\n",
        "import kerastuner as kt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu', input_shape=(input_shape,)))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(\n",
        "        optimizer=kt.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling='LOG')),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "tuner.search(x_train, y_train, validation_data=(x_test, y_test))\n",
        "```\n",
        "\n",
        "**<h2>5. Manual Tuning</h2>**\n",
        "\n",
        "Manual tuning involves manually experimenting with different hyperparameter values based on intuition and previous experience. This method is less systematic but can be useful for quick adjustments or when you have domain expertise.\n",
        "\n",
        "**<h2>Summary</h2>**\n",
        "\n",
        "Hyperparameter tuning is essential for optimizing machine learning models. Techniques like Grid Search, Random Search, Bayesian Optimization, and Hyperband offer various approaches to systematically explore and optimize hyperparameters. Each method has its strengths and is suited for different scenarios, from exhaustive search to efficient exploration of large hyperparameter spaces. Using libraries like Keras Tuner and Optuna can simplify and automate the tuning process."
      ],
      "metadata": {
        "id": "p-MLBegRqaIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grid search and random search with Keras**"
      ],
      "metadata": {
        "id": "sVpygClgrF0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Grid Search</h2>**"
      ],
      "metadata": {
        "id": "6DjK71TLrTww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "sFcRip0PrVvk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikeras"
      ],
      "metadata": {
        "id": "IUfCuKiLr4Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from scikeras.wrappers import KerasClassifier # KerasRegressor\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(32, 32, 3)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "QS5v7oRBraGQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
      ],
      "metadata": {
        "id": "mmHCOYknsSe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h2>Random Search</h2>**"
      ],
      "metadata": {
        "id": "8FjPcgCGsg2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(32, 32, 3)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)"
      ],
      "metadata": {
        "id": "Jus-dDo1sgF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "\n",
        "param_dist = {\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [10, 20],\n",
        "    'dropout_rate': uniform(0, 0.5)  # Example of a continuous distribution\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3)\n",
        "random_search_result = random_search.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")"
      ],
      "metadata": {
        "id": "-GUPbeTRsufH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}