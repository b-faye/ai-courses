{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IQpgkg0Jv1UX",
        "CCdyGtlqwELF",
        "HpB1qUoawVjY",
        "HePolVHCwdFz",
        "_2fUM9Wlwo9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Plan**\n",
        "\n",
        "**1. Basics of sequential data**\n",
        "\n",
        "**2. Introduction to RNNs and LSTM networks**\n",
        "\n",
        "**3. Building and training an LSTM model**\n",
        "\n"
      ],
      "metadata": {
        "id": "YiBklHRuTbj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basics of sequential data**"
      ],
      "metadata": {
        "id": "qFxsggyYfic8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential data refers to data where the order of the elements is important. This type of data is commonly found in various fields such as time series analysis, natural language processing, and speech recognition. Handling sequential data often requires specialized techniques and models due to its inherent temporal or sequential dependencies.\n",
        "\n",
        "**<h2>1. Characteristics of Sequential Data</h2>**\n",
        "\n",
        "- **Temporal Dependencies**: The current element in the sequence depends on previous elements.\n",
        "- **Order Matters**: Changing the order of elements alters the meaning or context.\n",
        "- **Variable Length**: Sequences can have different lengths, and they might not always be fixed.\n",
        "\n",
        "**<h2>2. Examples of Sequential Data</h2>**\n",
        "\n",
        "- **Time Series**: Stock prices, weather data, sensor readings.\n",
        "- **Text**: Sentences, paragraphs, or documents where the order of words matters.\n",
        "- **Speech**: Audio signals where phonemes and words follow a sequence.\n",
        "- **Sequences of Events**: User interactions, click streams, or transaction logs.\n",
        "\n",
        "**<h2>3. Models for Sequential Data</h2>**\n",
        "\n",
        "Sequential data requires models that can capture the dependencies between elements. Common models include:\n",
        "\n",
        "**<h2>a. Recurrent Neural Networks (RNNs)</h2>**\n",
        "\n",
        "RNNs are designed to handle sequential data by maintaining a hidden state that carries information from previous time steps.\n",
        "\n",
        "- **Vanilla RNNs**: Basic form of RNNs with simple connections, often struggling with long-term dependencies due to the vanishing gradient problem.\n",
        "  \n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=50, input_shape=(timesteps, features)))\n",
        "model.add(Dense(units=output_dim))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "```\n",
        "\n",
        "**<h2>b. Long Short-Term Memory (LSTM)</h2>**\n",
        "\n",
        "LSTMs are a type of RNN designed to better capture long-term dependencies by using gates to control the flow of information.\n",
        "\n",
        "```python\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, input_shape=(timesteps, features)))\n",
        "model.add(Dense(units=output_dim))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "```\n",
        "\n",
        "**<h2>c. Gated Recurrent Unit (GRU)</h2>**\n",
        "\n",
        "GRUs are similar to LSTMs but with a simpler architecture, often yielding similar performance with fewer parameters.\n",
        "\n",
        "```python\n",
        "from keras.layers import GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(GRU(units=50, input_shape=(timesteps, features)))\n",
        "model.add(Dense(units=output_dim))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "```\n",
        "\n",
        "**<h2>d. Transformers</h2>**\n",
        "\n",
        "Transformers use self-attention mechanisms to capture dependencies across the sequence without regard to distance. They are particularly effective for long sequences and are the basis of models like BERT and GPT.\n",
        "\n",
        "```python\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "\n",
        "# Example for using BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and prepare inputs\n",
        "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"tf\")\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "**<h2>4. Handling Variable-Length Sequences</h2>**\n",
        "\n",
        "When dealing with sequences of varying lengths, padding and masking are commonly used:\n",
        "\n",
        "- **Padding**: Adding dummy values to make all sequences in a batch the same length.\n",
        "- **Masking**: Informing the model which values are padding and should not be considered during training.\n",
        "\n",
        "```python\n",
        "from keras.layers import Masking\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0.0, input_shape=(max_timesteps, features)))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(units=output_dim))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "```\n",
        "\n",
        "**<h2>5. Evaluation and Metrics for Sequential Data</h2>**\n",
        "\n",
        "Evaluating models on sequential data often involves metrics that account for the temporal nature of the data:\n",
        "\n",
        "- **Accuracy**: For classification tasks.\n",
        "- **Mean Squared Error (MSE)**: For regression tasks.\n",
        "- **Precision, Recall, F1 Score**: For classification tasks, especially in imbalanced datasets.\n",
        "\n",
        "**<h2>6. Data Preparation for Sequential Models</h2>**\n",
        "\n",
        "- **Feature Engineering**: Extract relevant features that capture temporal dependencies.\n",
        "- **Normalization**: Scale features to improve model convergence.\n",
        "- **Splitting**: Divide data into training, validation, and test sets.\n",
        "\n",
        "**<h2>Example Workflow: Time Series Forecasting</h2>**\n",
        "\n",
        "1. **Load and preprocess data**:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data\n",
        "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
        "\n",
        "# Prepare input and output sequences\n",
        "def create_sequences(data, timesteps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - timesteps):\n",
        "        X.append(data[i:i + timesteps])\n",
        "        y.append(data[i + timesteps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "timesteps = 3\n",
        "X, y = create_sequences(data_scaled, timesteps)\n",
        "```\n",
        "\n",
        "2. **Build and train model**:\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(timesteps, 1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.fit(X, y, epochs=20)\n",
        "```\n",
        "\n",
        "3. **Evaluate and make predictions**:\n",
        "\n",
        "```python\n",
        "predictions = model.predict(X)\n",
        "```\n",
        "\n",
        "**<h2>Conclusion</h2>**\n",
        "\n",
        "Sequential data involves analyzing and modeling data where the order of elements is crucial. Various models such as RNNs, LSTMs, GRUs, and Transformers are tailored to handle these dependencies. Handling variable-length sequences, preprocessing data, and choosing appropriate evaluation metrics are key components in working with sequential data effectively."
      ],
      "metadata": {
        "id": "0KWZGHYlgAkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to RNNs and LSTM networks**"
      ],
      "metadata": {
        "id": "2hbgsmuvgwT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are powerful architectures designed for handling sequential data, where the order of elements is important. They are widely used in tasks such as time series forecasting, natural language processing, and speech recognition. Here’s an introduction to these networks:\n",
        "\n",
        "**<h2>Recurrent Neural Networks (RNNs)</h2>**\n",
        "\n",
        "**<h2>1. What is an RNN?</h2>**\n",
        "\n",
        "RNNs are neural networks designed to recognize patterns in sequences of data. Unlike traditional feedforward networks, RNNs have connections that form directed cycles, allowing information to persist. This cyclic structure enables RNNs to maintain a \"memory\" of previous inputs, which is crucial for tasks where context and sequence are important.\n",
        "\n",
        "**<h2>2. How RNNs Work</h2>**\n",
        "\n",
        "- **Architecture**: In an RNN, each unit (or neuron) is connected to the next unit in the sequence and also to itself. This allows information to flow through time.\n",
        "- **Hidden State**: RNNs maintain a hidden state that is updated at each time step based on the current input and the previous hidden state.\n",
        "- **Output**: The output at each time step is computed using the current hidden state and the current input.\n",
        "\n",
        "**<h2>3. Basic RNN Operation</h2>**\n",
        "\n",
        "At each time step $ t $:\n",
        "1. **Compute Hidden State**:\n",
        "   $$\n",
        "   h_t = \\text{tanh}(W_h \\cdot h_{t-1} + W_x \\cdot x_t + b_h)\n",
        "   $$\n",
        "   Where $ h_{t-1} $ is the hidden state from the previous time step, $ x_t $ is the input at time $ t $, $ W_h $ and $ W_x $ are weight matrices, and $ b_h $ is a bias term.\n",
        "   \n",
        "2. **Compute Output**:\n",
        "   $$\n",
        "   y_t = W_y \\cdot h_t + b_y\n",
        "   $$\n",
        "   Where $ W_y $ is the output weight matrix and $ b_y $ is the output bias.\n",
        "\n",
        "**<h2>4. Limitations of Basic RNNs</h2>**\n",
        "\n",
        "- **Vanishing Gradient Problem**: During training, gradients can become very small, making it difficult for the network to learn long-term dependencies.\n",
        "- **Exploding Gradient Problem**: Gradients can also grow excessively large, leading to unstable training.\n",
        "\n",
        "**<h2>Long Short-Term Memory (LSTM) Networks</h2>**\n",
        "\n",
        "**<h2>1. What is an LSTM?</h2>**\n",
        "\n",
        "LSTMs are a type of RNN designed to overcome the limitations of basic RNNs by better capturing long-term dependencies. They do this through a more complex architecture that includes gates to control the flow of information.\n",
        "\n",
        "**<h2>2. LSTM Architecture</h2>**\n",
        "\n",
        "An LSTM unit consists of:\n",
        "\n",
        "- **Cell State ($C_t$)**: This acts as a memory that can carry information across many time steps.\n",
        "- **Hidden State ($h_t$)**: This is the output of the LSTM unit and is used for making predictions.\n",
        "\n",
        "LSTMs use three types of gates:\n",
        "\n",
        "- **Forget Gate**: Decides which information from the cell state should be discarded.\n",
        "  $$\n",
        "  f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
        "  $$\n",
        "  \n",
        "- **Input Gate**: Decides which new information should be added to the cell state.\n",
        "  $$\n",
        "  i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
        "  $$\n",
        "  $$\n",
        "  \\tilde{C}_t = \\text{tanh}(W_c \\cdot [h_{t-1}, x_t] + b_c)\n",
        "  $$\n",
        "\n",
        "- **Output Gate**: Decides which information from the cell state should be output.\n",
        "  $$\n",
        "  o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
        "  $$\n",
        "  $$\n",
        "  h_t = o_t \\cdot \\text{tanh}(C_t)\n",
        "  $$\n",
        "\n",
        "Where:\n",
        "- $ \\sigma $ is the sigmoid activation function.\n",
        "- $ W_f, W_i, W_c, W_o $ are weight matrices.\n",
        "- $ b_f, b_i, b_c, b_o $ are bias terms.\n",
        "\n",
        "**<h2>3. How LSTMs Work</h2>**\n",
        "\n",
        "1. **Forget Gate**: Determine what information to discard from the cell state.\n",
        "2. **Input Gate**: Update the cell state with new information.\n",
        "3. **Update Cell State**: Combine the old cell state and the new information.\n",
        "4. **Output Gate**: Decide what the next hidden state should be based on the updated cell state.\n",
        "\n",
        "**<h2>4. Advantages of LSTMs</h2>**\n",
        "\n",
        "- **Long-Term Dependencies**: LSTMs are effective at learning long-term dependencies due to their gating mechanisms.\n",
        "- **Stability**: They are less affected by vanishing and exploding gradients compared to basic RNNs.\n",
        "\n",
        "**<h2>Example: Implementing an LSTM in Keras</h2>**\n",
        "\n",
        "Here is a simple example of building an LSTM model using Keras for a time series forecasting task:\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an LSTM layer\n",
        "model.add(LSTM(units=50, input_shape=(timesteps, features)))\n",
        "\n",
        "# Add a dense layer for output\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "**<h2>Summary</h2>**\n",
        "\n",
        "- **RNNs**: Useful for sequential data but can struggle with long-term dependencies due to issues like vanishing gradients.\n",
        "- **LSTMs**: An advanced type of RNN designed to handle long-term dependencies more effectively with a more complex architecture involving gates.\n",
        "\n",
        "Understanding and implementing RNNs and LSTMs is crucial for tasks involving sequential data where capturing temporal dependencies and context is important."
      ],
      "metadata": {
        "id": "j930A71XhPvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Example data\n",
        "# Generating some random data for demonstration\n",
        "timesteps = 10\n",
        "features = 1\n",
        "num_samples = 1000\n",
        "\n",
        "# Random data generation (replace with your actual data)\n",
        "X = np.random.rand(num_samples, timesteps, features)\n",
        "y = np.random.rand(num_samples, 1)"
      ],
      "metadata": {
        "id": "r3ZcLFROiRdb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first LSTM layer\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(timesteps, features)))\n",
        "\n",
        "# Add the second LSTM layer\n",
        "model.add(LSTM(units=50))\n",
        "\n",
        "# Add a Dense layer for output\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "NvK6srm-iVLU",
        "outputId": "4effc981-e7a6-4287-87df-beb0fe2f934e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │          \u001b[38;5;34m10,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,651\u001b[0m (119.73 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,651</span> (119.73 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,651\u001b[0m (119.73 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,651</span> (119.73 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Print training history\n",
        "print(history.history.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZekEqoqidg9",
        "outputId": "32c53daf-8db4-4f03-dfdc-45cd99fbc63a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.1868 - val_loss: 0.0957\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0897 - val_loss: 0.0832\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0861 - val_loss: 0.0820\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0869 - val_loss: 0.0811\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0870 - val_loss: 0.0835\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0884 - val_loss: 0.0833\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0925 - val_loss: 0.0817\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0835 - val_loss: 0.0831\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0839 - val_loss: 0.0800\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0869 - val_loss: 0.0837\n",
            "dict_keys(['loss', 'val_loss'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test data\n",
        "X_test = np.random.rand(num_samples, timesteps, features)\n",
        "y_test = np.random.rand(num_samples, 1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {test_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD-xAvuGiikq",
        "outputId": "f1a65811-3b9e-4a6e-a3b7-a3588aabcdf0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0815\n",
            "Test loss: 0.08456353098154068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building and training an LSTM model**"
      ],
      "metadata": {
        "id": "Fbt_pyE8izjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Example data\n",
        "timesteps = 10\n",
        "features = 1\n",
        "num_samples = 1000\n",
        "\n",
        "# Random data generation (replace with your actual data)\n",
        "X = np.random.rand(num_samples, timesteps, features)\n",
        "y = np.random.rand(num_samples, 1)\n"
      ],
      "metadata": {
        "id": "rK4PJTUjjC9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input layer\n",
        "inputs = Input(shape=(timesteps, features))\n",
        "\n",
        "# Add the first LSTM layer\n",
        "x = LSTM(units=50, return_sequences=True)(inputs)\n",
        "\n",
        "# Add the second LSTM layer\n",
        "x = LSTM(units=50)(x)\n",
        "\n",
        "# Add a Dense layer for output\n",
        "outputs = Dense(units=1)(x)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "qUklN2zdjDa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Print training history\n",
        "print(history.history.keys())\n"
      ],
      "metadata": {
        "id": "EBN_4D6xjGEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test data\n",
        "X_test = np.random.rand(num_samples, timesteps, features)\n",
        "y_test = np.random.rand(num_samples, 1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {test_loss}')"
      ],
      "metadata": {
        "id": "NAmC_j8JjKoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}